{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of transformer_contextual_lenses.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6JYlRZdKjtN",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jddAJf2QJqf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *** Comment out on cloud ***\n",
        "# Install the newest JAX and FLAX versions.\n",
        "!pip install --upgrade -q jax==0.1.61 jaxlib==0.1.42 flax==0.1.0rc2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEsdIj6XJ97M",
        "colab_type": "code",
        "outputId": "99a6d283-52d3-4f67-ed06-da69d6714b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# *** Comment out on cloud ***\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver_nightly'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grpc://10.40.38.18:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukKObz32J994",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "import itertools\n",
        "import os\n",
        "import time\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "from flax import nn\n",
        "from flax import optim\n",
        "from flax.metrics import tensorboard\n",
        "from flax.training import checkpoints\n",
        "from flax.training import common_utils\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "from jax import lax\n",
        "import jax.nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaurcJC1Pd1Y",
        "colab_type": "text"
      },
      "source": [
        "# Transformer model\n",
        "Code source: https://github.com/google/flax/blob/master/examples/lm1b/models.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FA6x8sNSvD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *** Warning: 0 represents PAD and EOS\n",
        "def shift_right(x):\n",
        "  \"\"\"Shift the input to the right by padding on axis 1.\"\"\"\n",
        "  pad_widths = [(0, 0)] * len(x.shape)\n",
        "  pad_widths[1] = (1, 0)  # Padding on axis=1\n",
        "  padded = jnp.pad(\n",
        "      x, pad_widths, mode='constant', constant_values=x.dtype.type(0))\n",
        "  return padded[:, :-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhlpIkvGTCN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embed(nn.Module):\n",
        "  \"\"\"Embedding Module.\n",
        "  A parameterized function from integers [0, n) to d-dimensional vectors.\n",
        "  \"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            num_embeddings,\n",
        "            features,\n",
        "            mode='input',\n",
        "            emb_init=nn.initializers.normal(stddev=1.0)):\n",
        "    \"\"\"Applies Embed module.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      num_embeddings: number of embedding\n",
        "      features: size of the embedding dimension\n",
        "      mode: either 'input' or 'output' -> to share input/output embedding\n",
        "      emb_init: embedding initializer\n",
        "    Returns:\n",
        "      output which is embedded input data\n",
        "    \"\"\"\n",
        "    embedding = self.param('embedding', (num_embeddings, features), emb_init)\n",
        "    if mode == 'input':\n",
        "      if inputs.dtype not in [jnp.int32, jnp.int64, jnp.uint32, jnp.uint64]:\n",
        "        raise ValueError('Input type must be an integer or unsigned integer.')\n",
        "      return jnp.take(embedding, inputs, axis=0)\n",
        "    if mode == 'output':\n",
        "      return jnp.einsum('bld,vd->blv', inputs, embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeZlvb5IT0Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sinusoidal_init(max_len=2048):\n",
        "  \"\"\"1D Sinusoidal Position Embedding Initializer.\n",
        "  Args:\n",
        "      max_len: maximum possible length for the input\n",
        "  Returns:\n",
        "      output: init function returning `(1, max_len, d_feature)`\n",
        "  \"\"\"\n",
        "\n",
        "  def init(key, shape, dtype=np.float32):\n",
        "    \"\"\"Sinusoidal init.\"\"\"\n",
        "    del key, dtype\n",
        "    d_feature = shape[-1]\n",
        "    pe = np.zeros((max_len, d_feature), dtype=np.float32)\n",
        "    position = np.arange(0, max_len)[:, np.newaxis]\n",
        "    div_term = np.exp(\n",
        "        np.arange(0, d_feature, 2) * -(np.log(10000.0) / d_feature))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    pe = pe[np.newaxis, :, :]  # [1, max_len, d_feature]\n",
        "    return jnp.array(pe)\n",
        "\n",
        "  return init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL7EOFcLT0W5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AddPositionEmbs(nn.Module):\n",
        "  \"\"\"Adds learned positional embeddings to the inputs.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            max_len=2048,\n",
        "            posemb_init=nn.initializers.normal(stddev=1.0),\n",
        "            cache=None):\n",
        "    \"\"\"Applies AddPositionEmbs module.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      max_len: maximum possible length for the input\n",
        "      posemb_init: positional embedding initializer\n",
        "      cache: flax attention cache for fast decoding.\n",
        "    Returns:\n",
        "      output: `(bs, timesteps, in_dim)`\n",
        "    \"\"\"\n",
        "    assert inputs.ndim == 3, ('Number of dimensions should be 3,'\n",
        "                              ' but it is: %d' % inputs.ndim)\n",
        "    length = inputs.shape[1]\n",
        "    pos_emb_shape = (1, max_len, inputs.shape[-1])\n",
        "    pos_embedding = self.param('pos_embedding', pos_emb_shape, posemb_init)\n",
        "    pe = pos_embedding[:, :length, :]\n",
        "    # We abuse the same attention Cache mechanism to run positional embeddings\n",
        "    # in fast predict mode. We could use state variables instead, but this\n",
        "    # simplifies invocation with a single top-level cache context manager.\n",
        "    # We only use the cache's position index for tracking decoding position.\n",
        "    if cache:\n",
        "      if self.is_initializing():\n",
        "        cache.store(lambda: (4, (1, 1)))\n",
        "      else:\n",
        "        cache_entry = cache.retrieve(None)\n",
        "        i = cache_entry.i\n",
        "        one = jnp.array(1, jnp.uint32)\n",
        "        cache_entry = cache_entry.replace(i=cache_entry.i + one)\n",
        "        cache.store(cache_entry)\n",
        "        _, _, df = pos_embedding.shape\n",
        "        pe = lax.dynamic_slice(pos_embedding, jnp.array((0, i, 0)),\n",
        "                               jnp.array((1, 1, df)))\n",
        "    return inputs + pe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nmqhnGGVIba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MlpBlock(nn.Module):\n",
        "  \"\"\"Transformer MLP block.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            mlp_dim,\n",
        "            out_dim=None,\n",
        "            dropout_rate=0.1,\n",
        "            deterministic=False,\n",
        "            kernel_init=nn.initializers.xavier_uniform(),\n",
        "            bias_init=nn.initializers.normal(stddev=1e-6)):\n",
        "    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n",
        "    actual_out_dim = inputs.shape[-1] if out_dim is None else out_dim\n",
        "    x = nn.Dense(inputs, mlp_dim, kernel_init=kernel_init, bias_init=bias_init)\n",
        "    x = nn.gelu(x)\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)\n",
        "    output = nn.Dense(\n",
        "        x, actual_out_dim, kernel_init=kernel_init, bias_init=bias_init)\n",
        "    output = nn.dropout(output, rate=dropout_rate, deterministic=deterministic)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHmusZUHVQAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer1DBlock(nn.Module):\n",
        "  \"\"\"Transformer layer (https://openreview.net/forum?id=H1e5GJBtDr).\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            qkv_dim,\n",
        "            mlp_dim,\n",
        "            num_heads,\n",
        "            causal_mask=False,\n",
        "            padding_mask=None,\n",
        "            dropout_rate=0.1,\n",
        "            attention_dropout_rate=0.1,\n",
        "            deterministic=False,\n",
        "            cache=None):\n",
        "    \"\"\"Applies Transformer1DBlock module.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      qkv_dim: dimension of the query/key/value\n",
        "      mlp_dim: dimension of the mlp on top of attention block\n",
        "      num_heads: number of heads\n",
        "      causal_mask: bool, mask future or not\n",
        "      padding_mask: bool, mask padding tokens\n",
        "      dropout_rate: dropout rate\n",
        "      attention_dropout_rate: dropout rate for attention weights\n",
        "      deterministic: bool, deterministic or not (to apply dropout)\n",
        "      cache: flax autoregressive cache for fast decoding.\n",
        "    Returns:\n",
        "      output after transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    # Attention block.\n",
        "    assert inputs.ndim == 3\n",
        "    x = nn.LayerNorm(inputs)\n",
        "    x = nn.SelfAttention(\n",
        "        x,\n",
        "        num_heads=num_heads,\n",
        "        qkv_features=qkv_dim,\n",
        "        attention_axis=(1,),\n",
        "        causal_mask=causal_mask,\n",
        "        padding_mask=padding_mask,\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.normal(stddev=1e-6),\n",
        "        bias=False,\n",
        "        broadcast_dropout=False,\n",
        "        dropout_rate=attention_dropout_rate,\n",
        "        deterministic=deterministic,\n",
        "        cache=cache)\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)\n",
        "    x = x + inputs\n",
        "\n",
        "    # MLP block.\n",
        "    y = nn.LayerNorm(x)\n",
        "    y = MlpBlock(\n",
        "        y,\n",
        "        mlp_dim=mlp_dim,\n",
        "        dropout_rate=dropout_rate,\n",
        "        deterministic=deterministic)\n",
        "\n",
        "    return x + y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVbX9CupVIhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yJSyWhhE2U7",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hatWMbzmErS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train_steps = 500000      # Max number of training steps.\n",
        "eval_frequency = 1000         # How often to run model evaluation.\n",
        "num_eval_steps = 20           # Number of steps to take during evaluation.\n",
        "random_seed = 0               # JAX PRNG random seed.\n",
        "learning_rate = 0.05          # Base learning rate.\n",
        "weight_decay = 1e-1           # AdamW-style relative weight decay factor.\n",
        "batch_size = 256              # \"Target\" Batch size.\n",
        "max_target_length = 256       # Maximum input length.\n",
        "max_eval_target_length = 256  # Maximum eval-set input length.\n",
        "\n",
        "lm_emb_dim = 512              # LM initial token embedding dimension.\n",
        "lm_num_heads = 8              # Number of heads in decoder layers.\n",
        "lm_num_layers = 6             # Number of decoder layers.\n",
        "lm_qkv_dim = 512              # Decoder query/key/value depth.\n",
        "lm_mlp_dim = 2048             # Feedforward (MLP) layer depth.\n",
        "\n",
        "rep_size = 256                 # Size of learned linear representation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OxG_5lZh6JZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 20\n",
        "input_shape = (batch_size, max_target_length)\n",
        "\n",
        "transformer_kwargs = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'rep_size' : rep_size,\n",
        "    'emb_dim': lm_emb_dim,\n",
        "    'num_heads': lm_num_heads,\n",
        "    'num_layers': lm_num_layers,\n",
        "    'qkv_dim': lm_qkv_dim,\n",
        "    'mlp_dim': lm_mlp_dim,\n",
        "    'max_len': max(max_target_length, max_eval_target_length)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mePIEKElffK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Init PRNG Stream.\n",
        "rng = random.PRNGKey(random_seed)\n",
        "rng, init_rng = random.split(rng)\n",
        "# We init the first set of dropout PRNG keys, but update it afterwards inside\n",
        "# the main pmap'd training update for performance.\n",
        "dropout_rngs = random.split(rng, jax.local_device_count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0mCSHSZiCWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a random sequence\n",
        "random_seq_len = 32\n",
        "random_seq = jnp.array([[np.random.randint(vocab_size) for _ in range(random_seq_len)] for __ in range(batch_size)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpLcARYIV0nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdyyxXnjnXh5",
        "colab_type": "text"
      },
      "source": [
        "# Transformer representation language model with lenses\n",
        "Code source: https://github.com/google/flax/blob/master/examples/lm1b/models.py \n",
        "\n",
        "Based on: https://arxiv.org/pdf/2002.08866.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIhGe9eOlZQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_pool(x, padding_mask, rep_size):\n",
        "   # Apply padding and subtract large number from padded indices\n",
        "   # Take maximum over sequence length\n",
        "   x = x * padding_mask\n",
        "   # may need to increase 999\n",
        "   neg_mask = - 999 * (1 - padding_mask)\n",
        "   x = x + neg_mask\n",
        "   rep = jnp.max(x, axis=1)\n",
        "   return rep\n",
        "\n",
        "\n",
        "def mean_pool(x, padding_mask, rep_size):\n",
        "  # Tak average over sequence length\n",
        "  x = x * padding_mask\n",
        "  rep = jnp.sum(x*padding_mask, axis=1) / jnp.sum(padding_mask, axis=1)\n",
        "  return rep\n",
        "\n",
        "\n",
        "def linearmax_pool(x, padding_mask, rep_size):\n",
        "  # Apply linear transformation + ReLU\n",
        "  # Apply padding\n",
        "  # Take maximum over sequence length\n",
        "  x = nn.Dense(\n",
        "        x,\n",
        "        rep_size,\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.normal(stddev=1e-6))\n",
        "  x = nn.relu(x)\n",
        "  x = x * padding_mask\n",
        "  rep = jnp.max(x, axis=1)\n",
        "  return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTaW3Apphk4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerRepresentationLM(nn.Module):\n",
        "  \"\"\"Transformer Model for language modeling and generating representations.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            vocab_size,\n",
        "            emb_dim=512,\n",
        "            num_heads=8,\n",
        "            num_layers=6,\n",
        "            qkv_dim=512,\n",
        "            mlp_dim=2048,\n",
        "            max_len=2048,\n",
        "            rep_size=256,\n",
        "            train=False,\n",
        "            shift=True,\n",
        "            dropout_rate=0.1,\n",
        "            attention_dropout_rate=0.1,\n",
        "            cache=None,\n",
        "            reduce_fn=None):\n",
        "    \"\"\"Applies Transformer model on the inputs.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      vocab_size: size of the vocabulary\n",
        "      emb_dim: dimension of embedding\n",
        "      num_heads: number of heads\n",
        "      num_layers: number of layers\n",
        "      qkv_dim: dimension of the query/key/value\n",
        "      mlp_dim: dimension of the mlp on top of attention block\n",
        "      max_len: maximum length.\n",
        "      train: bool: if model is training.\n",
        "      shift: bool: if we right-shift input - this is only disabled for\n",
        "        fast, looped single-token autoregressive decoding.\n",
        "      dropout_rate: dropout rate\n",
        "      attention_dropout_rate: dropout rate for attention weights\n",
        "      cache: flax autoregressive cache for fast decoding.\n",
        "    Returns:\n",
        "      output of a transformer decoder.\n",
        "    \"\"\"\n",
        "    padding_mask = jnp.where(inputs > 0, 1, 0).astype(jnp.float32)[..., None]\n",
        "    assert inputs.ndim == 2  # (batch, len)\n",
        "    x = inputs\n",
        "    if shift:\n",
        "      x = shift_right(x)\n",
        "    x = x.astype('int32')\n",
        "\n",
        "    x = Embed(x, num_embeddings=vocab_size, features=emb_dim, name='embed')\n",
        "\n",
        "    x = AddPositionEmbs(\n",
        "        x, max_len=max_len, posemb_init=sinusoidal_init(max_len=max_len),\n",
        "        cache=cache)\n",
        "\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=not train)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      x = Transformer1DBlock(\n",
        "          x,\n",
        "          qkv_dim=qkv_dim,\n",
        "          mlp_dim=mlp_dim,\n",
        "          num_heads=num_heads,\n",
        "          causal_mask=True,\n",
        "          padding_mask=padding_mask,\n",
        "          dropout_rate=dropout_rate,\n",
        "          attention_dropout_rate=attention_dropout_rate,\n",
        "          deterministic=not train,\n",
        "          cache=cache,\n",
        "      )\n",
        "    \n",
        "    if reduce_fn is None:\n",
        "      x = nn.LayerNorm(x)\n",
        "\n",
        "      logits = nn.Dense(\n",
        "          x,\n",
        "          vocab_size,\n",
        "          kernel_init=nn.initializers.xavier_uniform(),\n",
        "          bias_init=nn.initializers.normal(stddev=1e-6))\n",
        "\n",
        "      return logits\n",
        "\n",
        "    else:\n",
        "      rep = reduce_fn(x, padding_mask, rep_size)\n",
        "      return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upE0tNVviZXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIXn8icyiaMI",
        "colab_type": "text"
      },
      "source": [
        "# Testing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsbHGJMLhk_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_language_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Language Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerRepresentationLM.partial(**model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_maxpool_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Representation Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerRepresentationLM.partial(reduce_fn=max_pool, **model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_meanpool_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Representation Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerRepresentationLM.partial(reduce_fn=mean_pool, **model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_linearmaxpool_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Representation Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerRepresentationLM.partial(reduce_fn=linearmax_pool, **model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLDJCHrKcJw-",
        "colab_type": "code",
        "outputId": "42bc4e14-43b1-44cd-cd83-ce8b9d203c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# language model\n",
        "language_model, cache_def = create_language_model(init_rng, input_shape, transformer_kwargs)\n",
        "logits = language_model(random_seq)\n",
        "logits, logits.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[[-0.4531979 , -1.3108684 ,  2.1032448 , ..., -1.0460795 ,\n",
              "                -1.6000499 , -2.6734672 ],\n",
              "               [-1.7777493 , -2.1520298 ,  3.1351213 , ..., -0.07814984,\n",
              "                -2.9700904 , -1.6300236 ],\n",
              "               [-1.616412  ,  0.6506216 ,  2.820395  , ...,  0.6376484 ,\n",
              "                -1.6303223 ,  0.08634127],\n",
              "               ...,\n",
              "               [-0.5624157 , -1.5170264 ,  0.10607828, ...,  0.09684221,\n",
              "                 0.8825152 , -0.03843315],\n",
              "               [ 0.69813025, -1.0096921 ,  0.9634205 , ...,  0.98593485,\n",
              "                 1.3334614 , -1.0186895 ],\n",
              "               [ 0.7359726 , -1.9613379 ,  1.5075011 , ...,  0.5611275 ,\n",
              "                 2.176397  ,  0.88854706]],\n",
              " \n",
              "              [[-0.8542739 , -2.1835873 ,  1.1506859 , ..., -0.39479688,\n",
              "                -0.15798599,  0.19075663],\n",
              "               [-0.4297071 , -1.1878299 ,  2.328003  , ..., -1.0692949 ,\n",
              "                -1.4896861 , -2.6758325 ],\n",
              "               [ 0.7524646 , -0.36774924,  1.6349064 , ..., -0.65456676,\n",
              "                -2.3211255 , -1.7372403 ],\n",
              "               ...,\n",
              "               [-1.1945971 , -0.6399748 ,  0.11327802, ...,  1.0486921 ,\n",
              "                -0.09228302,  2.2770557 ],\n",
              "               [-1.3393428 , -1.1592482 ,  0.72010994, ..., -0.0365193 ,\n",
              "                 0.6343205 , -0.23936224],\n",
              "               [-0.43082747, -2.2710347 ,  2.3909118 , ..., -0.07184555,\n",
              "                -0.705473  , -1.1354221 ]],\n",
              " \n",
              "              [[-0.4531979 , -1.3108684 ,  2.1032448 , ..., -1.0460795 ,\n",
              "                -1.6000499 , -2.6734672 ],\n",
              "               [ 0.02053863, -0.36100364,  1.817394  , ..., -0.6699647 ,\n",
              "                -0.8985394 , -2.5614614 ],\n",
              "               [-1.3896749 ,  0.32347822,  1.725017  , ...,  0.04450657,\n",
              "                -1.4227079 ,  0.27616087],\n",
              "               ...,\n",
              "               [-1.3722327 , -0.70027155,  1.5412421 , ...,  1.0464371 ,\n",
              "                 0.13054454, -0.99044806],\n",
              "               [ 0.97014284, -2.3759758 ,  2.4244714 , ...,  1.0616846 ,\n",
              "                 1.7923203 ,  0.84487337],\n",
              "               [ 0.2006107 , -2.1283271 ,  2.0955517 , ...,  0.7286039 ,\n",
              "                 0.7666109 , -0.69802326]],\n",
              " \n",
              "              ...,\n",
              " \n",
              "              [[-0.4531979 , -1.3108684 ,  2.1032448 , ..., -1.0460795 ,\n",
              "                -1.6000499 , -2.6734672 ],\n",
              "               [-0.754165  , -0.8091302 ,  0.82960725, ..., -0.58821416,\n",
              "                -3.755621  , -1.0020217 ],\n",
              "               [-0.82791334, -0.47947764,  3.4063413 , ...,  0.4616503 ,\n",
              "                -3.986454  , -0.17517398],\n",
              "               ...,\n",
              "               [-1.5456901 , -1.4640499 ,  1.0027115 , ...,  0.19674252,\n",
              "                -0.10245097,  0.1660526 ],\n",
              "               [-1.2371461 , -2.0121052 ,  1.7334656 , ...,  0.3490706 ,\n",
              "                -0.84417033,  1.1069739 ],\n",
              "               [-0.6036572 , -2.3599799 ,  2.115447  , ...,  1.2592436 ,\n",
              "                 0.60555834,  0.5813408 ]],\n",
              " \n",
              "              [[-0.4531979 , -1.3108684 ,  2.1032448 , ..., -1.0460795 ,\n",
              "                -1.6000499 , -2.6734672 ],\n",
              "               [-0.9510298 , -1.3589438 ,  1.2226273 , ..., -0.15708502,\n",
              "                -0.25263196, -0.02471272],\n",
              "               [-0.2536549 , -1.0779378 ,  2.2450414 , ..., -1.271887  ,\n",
              "                -1.5696868 , -2.6180553 ],\n",
              "               ...,\n",
              "               [-0.06725808, -0.92460835,  1.4468079 , ...,  1.0762299 ,\n",
              "                 1.3241199 , -0.5774385 ],\n",
              "               [-1.0918578 , -2.7978492 ,  0.78005123, ...,  0.45333076,\n",
              "                 1.6336852 , -0.49744758],\n",
              "               [-0.21840313, -1.8004099 ,  2.8855329 , ..., -0.05155141,\n",
              "                -0.7248697 , -1.0572278 ]],\n",
              " \n",
              "              [[-0.4531979 , -1.3108684 ,  2.1032448 , ..., -1.0460795 ,\n",
              "                -1.6000499 , -2.6734672 ],\n",
              "               [-0.9205468 , -0.45961416,  3.0508454 , ..., -0.7318555 ,\n",
              "                -4.4677753 , -1.4733328 ],\n",
              "               [ 0.4969408 , -2.7255561 ,  1.9082718 , ..., -0.34268755,\n",
              "                -1.4583379 ,  0.7587647 ],\n",
              "               ...,\n",
              "               [ 1.0225883 , -3.129109  ,  2.171928  , ...,  0.60068655,\n",
              "                 0.9514785 ,  0.26370233],\n",
              "               [ 0.23728946, -3.1605022 ,  2.1905687 , ...,  0.23927523,\n",
              "                 0.41338128, -1.0912749 ],\n",
              "               [ 0.90775895, -1.931499  ,  1.9373736 , ...,  1.0341984 ,\n",
              "                 0.62499714, -1.2352182 ]]], dtype=float32), (256, 32, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auuj0BNZbYrX",
        "colab_type": "code",
        "outputId": "dc580e7a-cc76-411c-8ea0-5b4909130ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "# max pool representation\n",
        "maxpool_model, cache_def = create_maxpool_model(init_rng, input_shape, transformer_kwargs)\n",
        "maxpool_rep = maxpool_model(random_seq)\n",
        "maxpool_rep, maxpool_rep.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[5.8016844, 1.9811082, 1.8710039, ..., 2.8121529, 3.2546961,\n",
              "               1.2917032],\n",
              "              [5.6697426, 1.9579171, 2.778206 , ..., 4.0522223, 3.2326026,\n",
              "               1.4650214],\n",
              "              [6.437824 , 1.7363436, 2.8137958, ..., 2.8567169, 0.8620118,\n",
              "               2.0121317],\n",
              "              ...,\n",
              "              [6.2676773, 4.4628067, 1.5246924, ..., 2.4786766, 2.2512991,\n",
              "               1.641068 ],\n",
              "              [6.6310844, 2.24179  , 2.7288253, ..., 1.3769737, 2.9103465,\n",
              "               1.5294666],\n",
              "              [7.3754625, 1.9371665, 2.3507214, ..., 3.3634698, 1.8894832,\n",
              "               3.5483205]], dtype=float32), (256, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb-0xsC6MFda",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "a460c60b-370a-4784-ad1e-b856ffca5b0c"
      },
      "source": [
        "# mean pool representation\n",
        "meanpool_model, cache_def = create_meanpool_model(init_rng, input_shape, transformer_kwargs)\n",
        "meanpool_rep = meanpool_model(random_seq)\n",
        "meanpool_rep, meanpool_rep.shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[ 3.258283  , -0.78890467, -0.7485733 , ..., -0.0262435 ,\n",
              "               -0.0446901 , -0.5474629 ],\n",
              "              [ 2.7003772 , -0.17053264, -0.05264667, ...,  0.41636118,\n",
              "                0.9873389 , -1.5707717 ],\n",
              "              [ 3.5848014 , -0.6702391 ,  0.31737843, ...,  0.41866228,\n",
              "               -0.9229967 , -1.1717583 ],\n",
              "              ...,\n",
              "              [ 3.0688536 ,  1.4983838 , -1.0687525 , ...,  0.5425056 ,\n",
              "                0.25565332, -1.4005684 ],\n",
              "              [ 3.3800244 , -0.49530128, -0.01721719, ..., -0.58174795,\n",
              "                0.10043278, -0.6288551 ],\n",
              "              [ 3.5419006 ,  0.04537945, -0.98715705, ..., -0.18438677,\n",
              "                0.01979696,  0.18434958]], dtype=float32), (256, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDtHZM4Lc5Ey",
        "colab_type": "code",
        "outputId": "f4683b80-d324-4ba1-a073-318cb5f8f98b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "# linear + ReLU + max pool representation\n",
        "linearmaxpool_model, cache_def = create_linearmaxpool_model(init_rng, input_shape, transformer_kwargs)\n",
        "linearmaxpool_rep = linearmaxpool_model(random_seq)\n",
        "linearmaxpool_rep, linearmaxpool_rep.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[ 2.8250415 ,  2.358769  ,  7.9191527 , ...,  1.4483124 ,\n",
              "                3.4595916 , 10.2986355 ],\n",
              "              [ 1.8786578 ,  2.6686628 ,  5.8341017 , ...,  1.3040167 ,\n",
              "                4.6978183 ,  9.436666  ],\n",
              "              [ 0.8359105 ,  6.006627  ,  5.065858  , ...,  1.9864584 ,\n",
              "                4.396874  , 10.2986355 ],\n",
              "              ...,\n",
              "              [ 0.46043354,  2.550679  ,  6.09299   , ...,  2.4697723 ,\n",
              "                3.329556  , 10.2986355 ],\n",
              "              [ 0.        ,  2.896603  ,  5.93247   , ...,  0.79770863,\n",
              "                3.4861722 , 10.2986355 ],\n",
              "              [ 4.249893  ,  1.5436288 ,  5.1353054 , ...,  0.13533713,\n",
              "                4.108497  , 10.2986355 ]], dtype=float32), (256, 256))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WspJK-hc7ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}