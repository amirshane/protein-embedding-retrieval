{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of transformer_contextual_lenses.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6JYlRZdKjtN",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jddAJf2QJqf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *** Comment out on cloud ***\n",
        "# Install the newest JAX and FLAX versions.\n",
        "!pip install --upgrade -q jax==0.1.61 jaxlib==0.1.42 flax==0.1.0rc2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEsdIj6XJ97M",
        "colab_type": "code",
        "outputId": "7c70483f-fe1e-4b5e-f25b-ee3eca9adbbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# *** Comment out on cloud ***\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver_nightly'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grpc://10.40.38.18:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukKObz32J994",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "import itertools\n",
        "import os\n",
        "import time\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "from flax import nn\n",
        "from flax import optim\n",
        "from flax.metrics import tensorboard\n",
        "from flax.training import checkpoints\n",
        "from flax.training import common_utils\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "from jax import lax\n",
        "import jax.nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaurcJC1Pd1Y",
        "colab_type": "text"
      },
      "source": [
        "# Transformer model\n",
        "Code source: https://github.com/google/flax/blob/master/examples/lm1b/models.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FA6x8sNSvD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *** Warning: 0 represents PAD and EOS\n",
        "def shift_right(x):\n",
        "  \"\"\"Shift the input to the right by padding on axis 1.\"\"\"\n",
        "  pad_widths = [(0, 0)] * len(x.shape)\n",
        "  pad_widths[1] = (1, 0)  # Padding on axis=1\n",
        "  padded = jnp.pad(\n",
        "      x, pad_widths, mode='constant', constant_values=x.dtype.type(0))\n",
        "  return padded[:, :-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhlpIkvGTCN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embed(nn.Module):\n",
        "  \"\"\"Embedding Module.\n",
        "  A parameterized function from integers [0, n) to d-dimensional vectors.\n",
        "  \"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            num_embeddings,\n",
        "            features,\n",
        "            mode='input',\n",
        "            emb_init=nn.initializers.normal(stddev=1.0)):\n",
        "    \"\"\"Applies Embed module.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      num_embeddings: number of embedding\n",
        "      features: size of the embedding dimension\n",
        "      mode: either 'input' or 'output' -> to share input/output embedding\n",
        "      emb_init: embedding initializer\n",
        "    Returns:\n",
        "      output which is embedded input data\n",
        "    \"\"\"\n",
        "    embedding = self.param('embedding', (num_embeddings, features), emb_init)\n",
        "    if mode == 'input':\n",
        "      if inputs.dtype not in [jnp.int32, jnp.int64, jnp.uint32, jnp.uint64]:\n",
        "        raise ValueError('Input type must be an integer or unsigned integer.')\n",
        "      return jnp.take(embedding, inputs, axis=0)\n",
        "    if mode == 'output':\n",
        "      return jnp.einsum('bld,vd->blv', inputs, embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeZlvb5IT0Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sinusoidal_init(max_len=2048):\n",
        "  \"\"\"1D Sinusoidal Position Embedding Initializer.\n",
        "  Args:\n",
        "      max_len: maximum possible length for the input\n",
        "  Returns:\n",
        "      output: init function returning `(1, max_len, d_feature)`\n",
        "  \"\"\"\n",
        "\n",
        "  def init(key, shape, dtype=np.float32):\n",
        "    \"\"\"Sinusoidal init.\"\"\"\n",
        "    del key, dtype\n",
        "    d_feature = shape[-1]\n",
        "    pe = np.zeros((max_len, d_feature), dtype=np.float32)\n",
        "    position = np.arange(0, max_len)[:, np.newaxis]\n",
        "    div_term = np.exp(\n",
        "        np.arange(0, d_feature, 2) * -(np.log(10000.0) / d_feature))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    pe = pe[np.newaxis, :, :]  # [1, max_len, d_feature]\n",
        "    return jnp.array(pe)\n",
        "\n",
        "  return init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL7EOFcLT0W5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AddPositionEmbs(nn.Module):\n",
        "  \"\"\"Adds learned positional embeddings to the inputs.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            max_len=2048,\n",
        "            posemb_init=nn.initializers.normal(stddev=1.0),\n",
        "            cache=None):\n",
        "    \"\"\"Applies AddPositionEmbs module.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      max_len: maximum possible length for the input\n",
        "      posemb_init: positional embedding initializer\n",
        "      cache: flax attention cache for fast decoding.\n",
        "    Returns:\n",
        "      output: `(bs, timesteps, in_dim)`\n",
        "    \"\"\"\n",
        "    assert inputs.ndim == 3, ('Number of dimensions should be 3,'\n",
        "                              ' but it is: %d' % inputs.ndim)\n",
        "    length = inputs.shape[1]\n",
        "    pos_emb_shape = (1, max_len, inputs.shape[-1])\n",
        "    pos_embedding = self.param('pos_embedding', pos_emb_shape, posemb_init)\n",
        "    pe = pos_embedding[:, :length, :]\n",
        "    # We abuse the same attention Cache mechanism to run positional embeddings\n",
        "    # in fast predict mode. We could use state variables instead, but this\n",
        "    # simplifies invocation with a single top-level cache context manager.\n",
        "    # We only use the cache's position index for tracking decoding position.\n",
        "    if cache:\n",
        "      if self.is_initializing():\n",
        "        cache.store(lambda: (4, (1, 1)))\n",
        "      else:\n",
        "        cache_entry = cache.retrieve(None)\n",
        "        i = cache_entry.i\n",
        "        one = jnp.array(1, jnp.uint32)\n",
        "        cache_entry = cache_entry.replace(i=cache_entry.i + one)\n",
        "        cache.store(cache_entry)\n",
        "        _, _, df = pos_embedding.shape\n",
        "        pe = lax.dynamic_slice(pos_embedding, jnp.array((0, i, 0)),\n",
        "                               jnp.array((1, 1, df)))\n",
        "    return inputs + pe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nmqhnGGVIba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MlpBlock(nn.Module):\n",
        "  \"\"\"Transformer MLP block.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            mlp_dim,\n",
        "            out_dim=None,\n",
        "            dropout_rate=0.1,\n",
        "            deterministic=False,\n",
        "            kernel_init=nn.initializers.xavier_uniform(),\n",
        "            bias_init=nn.initializers.normal(stddev=1e-6)):\n",
        "    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n",
        "    actual_out_dim = inputs.shape[-1] if out_dim is None else out_dim\n",
        "    x = nn.Dense(inputs, mlp_dim, kernel_init=kernel_init, bias_init=bias_init)\n",
        "    x = nn.gelu(x)\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)\n",
        "    output = nn.Dense(\n",
        "        x, actual_out_dim, kernel_init=kernel_init, bias_init=bias_init)\n",
        "    output = nn.dropout(output, rate=dropout_rate, deterministic=deterministic)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHmusZUHVQAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer1DBlock(nn.Module):\n",
        "  \"\"\"Transformer layer (https://openreview.net/forum?id=H1e5GJBtDr).\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            qkv_dim,\n",
        "            mlp_dim,\n",
        "            num_heads,\n",
        "            causal_mask=False,\n",
        "            padding_mask=None,\n",
        "            dropout_rate=0.1,\n",
        "            attention_dropout_rate=0.1,\n",
        "            deterministic=False,\n",
        "            cache=None):\n",
        "    \"\"\"Applies Transformer1DBlock module.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      qkv_dim: dimension of the query/key/value\n",
        "      mlp_dim: dimension of the mlp on top of attention block\n",
        "      num_heads: number of heads\n",
        "      causal_mask: bool, mask future or not\n",
        "      padding_mask: bool, mask padding tokens\n",
        "      dropout_rate: dropout rate\n",
        "      attention_dropout_rate: dropout rate for attention weights\n",
        "      deterministic: bool, deterministic or not (to apply dropout)\n",
        "      cache: flax autoregressive cache for fast decoding.\n",
        "    Returns:\n",
        "      output after transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    # Attention block.\n",
        "    assert inputs.ndim == 3\n",
        "    x = nn.LayerNorm(inputs)\n",
        "    x = nn.SelfAttention(\n",
        "        x,\n",
        "        num_heads=num_heads,\n",
        "        qkv_features=qkv_dim,\n",
        "        attention_axis=(1,),\n",
        "        causal_mask=causal_mask,\n",
        "        padding_mask=padding_mask,\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.normal(stddev=1e-6),\n",
        "        bias=False,\n",
        "        broadcast_dropout=False,\n",
        "        dropout_rate=attention_dropout_rate,\n",
        "        deterministic=deterministic,\n",
        "        cache=cache)\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)\n",
        "    x = x + inputs\n",
        "\n",
        "    # MLP block.\n",
        "    y = nn.LayerNorm(x)\n",
        "    y = MlpBlock(\n",
        "        y,\n",
        "        mlp_dim=mlp_dim,\n",
        "        dropout_rate=dropout_rate,\n",
        "        deterministic=deterministic)\n",
        "\n",
        "    return x + y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVbX9CupVIhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yJSyWhhE2U7",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hatWMbzmErS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train_steps = 500000      # Max number of training steps.\n",
        "eval_frequency = 1000         # How often to run model evaluation.\n",
        "num_eval_steps = 20           # Number of steps to take during evaluation.\n",
        "random_seed = 0               # JAX PRNG random seed.\n",
        "learning_rate = 0.05          # Base learning rate.\n",
        "weight_decay = 1e-1           # AdamW-style relative weight decay factor.\n",
        "batch_size = 256              # \"Target\" Batch size.\n",
        "max_target_length = 256       # Maximum input length.\n",
        "max_eval_target_length = 256  # Maximum eval-set input length.\n",
        "\n",
        "lm_emb_dim = 512              # LM initial token embedding dimension.\n",
        "lm_num_heads = 8              # Number of heads in decoder layers.\n",
        "lm_num_layers = 6             # Number of decoder layers.\n",
        "lm_qkv_dim = 512              # Decoder query/key/value depth.\n",
        "lm_mlp_dim = 2048             # Feedforward (MLP) layer depth.\n",
        "\n",
        "rep_size = 256                 # Size of learned linear representation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OxG_5lZh6JZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 20\n",
        "input_shape = (batch_size, max_target_length)\n",
        "\n",
        "transformer_kwargs = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'rep_size' : rep_size,\n",
        "    'emb_dim': lm_emb_dim,\n",
        "    'num_heads': lm_num_heads,\n",
        "    'num_layers': lm_num_layers,\n",
        "    'qkv_dim': lm_qkv_dim,\n",
        "    'mlp_dim': lm_mlp_dim,\n",
        "    'max_len': max(max_target_length, max_eval_target_length)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mePIEKElffK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Init PRNG Stream.\n",
        "rng = random.PRNGKey(random_seed)\n",
        "rng, init_rng = random.split(rng)\n",
        "# We init the first set of dropout PRNG keys, but update it afterwards inside\n",
        "# the main pmap'd training update for performance.\n",
        "dropout_rngs = random.split(rng, jax.local_device_count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0mCSHSZiCWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a random sequence\n",
        "random_seq_len = 32\n",
        "random_seq = jnp.array([[np.random.randint(vocab_size) for _ in range(random_seq_len)] for __ in range(batch_size)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpLcARYIV0nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdyyxXnjnXh5",
        "colab_type": "text"
      },
      "source": [
        "# Transformer representation language model with lenses\n",
        "Code source: https://github.com/google/flax/blob/master/examples/lm1b/models.py \n",
        "\n",
        "Based on: https://arxiv.org/pdf/2002.08866.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIhGe9eOlZQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_pool(x, padding_mask, rep_size):\n",
        "   # Apply padding and subtract large number from padded indices\n",
        "   # Take maximum over sequence length\n",
        "   x = x * padding_mask\n",
        "   # may need to increase 999\n",
        "   neg_mask = - 999 * (1 - padding_mask)\n",
        "   x = x + neg_mask\n",
        "   rep = jnp.max(x, axis=1)\n",
        "   return rep\n",
        "\n",
        "\n",
        "def mean_pool(x, padding_mask, rep_size):\n",
        "  # Tak average over sequence length\n",
        "  x = x * padding_mask\n",
        "  rep = jnp.mean(x, axis=1)\n",
        "  return rep\n",
        "\n",
        "\n",
        "def linearmax_pool(x, padding_mask, rep_size):\n",
        "  # Apply linear transformation + ReLU\n",
        "  # Apply padding\n",
        "  # Take maximum over sequence length\n",
        "  x = nn.Dense(\n",
        "        x,\n",
        "        rep_size,\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.normal(stddev=1e-6))\n",
        "  x = nn.relu(x)\n",
        "  x = x * padding_mask\n",
        "  rep = jnp.max(x, axis=1)\n",
        "  return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTaW3Apphk4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerRepresentationLM(nn.Module):\n",
        "  \"\"\"Transformer Model for language modeling and generating representations.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            vocab_size,\n",
        "            emb_dim=512,\n",
        "            num_heads=8,\n",
        "            num_layers=6,\n",
        "            qkv_dim=512,\n",
        "            mlp_dim=2048,\n",
        "            max_len=2048,\n",
        "            rep_size=256,\n",
        "            train=False,\n",
        "            shift=True,\n",
        "            dropout_rate=0.1,\n",
        "            attention_dropout_rate=0.1,\n",
        "            cache=None,\n",
        "            reduce_fn=None):\n",
        "    \"\"\"Applies Transformer model on the inputs.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      vocab_size: size of the vocabulary\n",
        "      emb_dim: dimension of embedding\n",
        "      num_heads: number of heads\n",
        "      num_layers: number of layers\n",
        "      qkv_dim: dimension of the query/key/value\n",
        "      mlp_dim: dimension of the mlp on top of attention block\n",
        "      max_len: maximum length.\n",
        "      train: bool: if model is training.\n",
        "      shift: bool: if we right-shift input - this is only disabled for\n",
        "        fast, looped single-token autoregressive decoding.\n",
        "      dropout_rate: dropout rate\n",
        "      attention_dropout_rate: dropout rate for attention weights\n",
        "      cache: flax autoregressive cache for fast decoding.\n",
        "    Returns:\n",
        "      output of a transformer decoder.\n",
        "    \"\"\"\n",
        "    padding_mask = jnp.where(inputs > 0, 1, 0).astype(jnp.float32)[..., None]\n",
        "    assert inputs.ndim == 2  # (batch, len)\n",
        "    x = inputs\n",
        "    if shift:\n",
        "      x = shift_right(x)\n",
        "    x = x.astype('int32')\n",
        "\n",
        "    x = Embed(x, num_embeddings=vocab_size, features=emb_dim, name='embed')\n",
        "\n",
        "    x = AddPositionEmbs(\n",
        "        x, max_len=max_len, posemb_init=sinusoidal_init(max_len=max_len),\n",
        "        cache=cache)\n",
        "\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=not train)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      x = Transformer1DBlock(\n",
        "          x,\n",
        "          qkv_dim=qkv_dim,\n",
        "          mlp_dim=mlp_dim,\n",
        "          num_heads=num_heads,\n",
        "          causal_mask=True,\n",
        "          padding_mask=padding_mask,\n",
        "          dropout_rate=dropout_rate,\n",
        "          attention_dropout_rate=attention_dropout_rate,\n",
        "          deterministic=not train,\n",
        "          cache=cache,\n",
        "      )\n",
        "    \n",
        "    if reduce_fn is None:\n",
        "      x = nn.LayerNorm(x)\n",
        "\n",
        "      logits = nn.Dense(\n",
        "          x,\n",
        "          vocab_size,\n",
        "          kernel_init=nn.initializers.xavier_uniform(),\n",
        "          bias_init=nn.initializers.normal(stddev=1e-6))\n",
        "\n",
        "      return logits\n",
        "\n",
        "    else:\n",
        "      rep = reduce_fn(x, padding_mask, rep_size)\n",
        "      return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upE0tNVviZXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIXn8icyiaMI",
        "colab_type": "text"
      },
      "source": [
        "# Testing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsbHGJMLhk_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_language_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Language Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerRepresentationLM.partial(**model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_maxpool_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Representation Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerRepresentationLM.partial(reduce_fn=max_pool, **model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_meanpool_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Representation Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerRepresentationLM.partial(reduce_fn=mean_pool, **model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_linearmaxpool_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Representation Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerRepresentationLM.partial(reduce_fn=linearmax_pool, **model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLDJCHrKcJw-",
        "colab_type": "code",
        "outputId": "c2955cf8-ef11-4a45-dd51-cff4a6536393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# language model\n",
        "language_model, cache_def = create_language_model(init_rng, input_shape, transformer_kwargs)\n",
        "logits = language_model(random_seq)\n",
        "logits, logits.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-1.8233123e+00, -2.0055380e+00,  1.8289915e+00, ...,\n",
              "                -5.9956932e-01, -2.4272883e+00, -5.0563967e-01],\n",
              "               [-1.6366332e+00, -1.9137582e+00,  2.0673552e+00, ...,\n",
              "                -3.7543427e-02, -3.3339689e+00, -5.3289187e-01],\n",
              "               ...,\n",
              "               [-3.0803102e-01, -1.2399775e+00,  4.0920792e+00, ...,\n",
              "                 1.9676275e+00,  5.0824100e-01, -1.1728922e-01],\n",
              "               [ 1.4172210e+00, -2.5054238e+00,  1.0797907e+00, ...,\n",
              "                 5.6125474e-01,  5.4014224e-01, -1.5493642e+00],\n",
              "               [-3.2014316e-01, -2.4408956e+00,  1.8435308e+00, ...,\n",
              "                 1.1287520e+00,  5.3974998e-01, -5.6178880e-01]],\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-8.9981824e-01, -1.3519605e+00,  1.3797101e+00, ...,\n",
              "                 1.3774323e-02, -1.7791220e+00, -1.5944890e+00],\n",
              "               [-9.6677881e-01, -1.2879474e+00,  1.4130836e+00, ...,\n",
              "                 4.8741376e-01, -1.4189419e+00, -1.4606270e+00],\n",
              "               ...,\n",
              "               [-8.9103632e-02, -1.1181058e+00,  1.7784251e+00, ...,\n",
              "                 1.9590026e+00,  1.8992653e+00, -5.8205473e-01],\n",
              "               [-3.9428419e-01, -2.2550352e+00,  2.3584626e+00, ...,\n",
              "                 1.4041761e+00,  1.2339941e+00, -8.8077295e-01],\n",
              "               [-5.3812474e-01, -5.6662738e-01,  4.0494308e+00, ...,\n",
              "                 2.3574989e+00,  1.1078514e+00,  6.8109715e-01]],\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-6.1661464e-01, -1.2949111e+00,  2.4757829e+00, ...,\n",
              "                -1.3056217e+00, -9.0616316e-01,  1.3522960e-01],\n",
              "               [-6.7908727e-02,  5.7794936e-02,  1.7857573e+00, ...,\n",
              "                -7.2376385e-02,  2.3232400e-01, -7.2232693e-01],\n",
              "               ...,\n",
              "               [-4.9173301e-01, -7.4671602e-01,  3.8382711e+00, ...,\n",
              "                 1.0120407e+00, -2.6993164e-01,  1.7974266e-03],\n",
              "               [-2.9398182e-01, -2.0570500e+00,  2.6695766e+00, ...,\n",
              "                 5.5638582e-01,  3.8357025e-01, -1.0859929e+00],\n",
              "               [-6.9952041e-01, -2.8537219e+00,  1.7098886e+00, ...,\n",
              "                 4.3516266e-01,  6.6717899e-01, -8.3545321e-01]],\n",
              " \n",
              "              ...,\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-8.5583907e-01, -4.0283471e-01,  3.8011644e+00, ...,\n",
              "                 3.3138490e-01, -3.4278026e+00,  1.7776801e-01],\n",
              "               [-2.1915207e+00, -2.1556747e+00,  3.0472279e+00, ...,\n",
              "                 5.5541408e-01, -3.0975909e+00, -7.8593457e-01],\n",
              "               ...,\n",
              "               [-1.1427557e+00, -1.8955865e+00,  2.1336849e+00, ...,\n",
              "                 7.5389922e-01,  6.2949598e-01,  4.2076442e-02],\n",
              "               [-1.0879430e+00, -1.6082757e+00,  1.7850844e+00, ...,\n",
              "                 7.9675776e-01, -6.9155824e-01, -2.2269013e+00],\n",
              "               [-4.1676547e-02, -2.7421029e+00,  3.3837132e+00, ...,\n",
              "                 8.0120862e-01,  4.4651294e-01,  2.0711984e-01]],\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [ 5.4261732e-01, -4.2708763e-01,  1.1695975e+00, ...,\n",
              "                -7.6125503e-01, -2.3720825e+00, -1.5303682e+00],\n",
              "               [-1.4147294e+00, -2.0045159e+00,  3.1804845e+00, ...,\n",
              "                -1.6636981e-01, -3.2193646e+00, -1.4119180e+00],\n",
              "               ...,\n",
              "               [ 1.7594266e-01, -1.0100365e+00, -4.2798227e-01, ...,\n",
              "                 3.0170904e-02,  8.4830296e-01,  3.1140757e-01],\n",
              "               [-1.7258704e+00, -6.2436146e-01,  7.9343188e-01, ...,\n",
              "                 4.0892833e-01,  5.4680389e-01,  2.8756675e-01],\n",
              "               [-1.5786557e+00, -1.4535638e+00,  8.3687496e-01, ...,\n",
              "                 7.3588383e-01,  7.4352932e-01,  6.5461791e-01]],\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-2.2059345e+00, -1.3470999e+00,  2.1907320e+00, ...,\n",
              "                -1.5825154e+00, -1.9402893e+00,  5.2538699e-01],\n",
              "               [-1.7543170e+00, -3.6541289e-01,  2.8188183e+00, ...,\n",
              "                -3.1583855e-01, -3.0196414e+00, -5.0866175e-01],\n",
              "               ...,\n",
              "               [-9.6026558e-01, -3.0329823e+00,  7.6125348e-01, ...,\n",
              "                 7.7340949e-01,  1.3383216e+00, -8.4345169e-02],\n",
              "               [ 6.7000335e-01, -2.6326194e+00,  2.0649412e+00, ...,\n",
              "                 1.0347949e+00,  2.0475554e+00,  1.3422427e+00],\n",
              "               [-3.2676661e-01, -1.3201776e+00,  2.4568589e+00, ...,\n",
              "                 6.9368213e-01,  1.4552257e+00,  7.6799415e-02]]],            dtype=float32),\n",
              " (256, 32, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXcJhL6lcKY1",
        "colab_type": "code",
        "outputId": "494928da-9fef-4097-e124-11ba01bef315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "# mean pool representation\n",
        "meanpool_model, cache_def = create_meanpool_model(init_rng, input_shape, transformer_kwargs)\n",
        "meanpool_rep = meanpool_model(random_seq)\n",
        "meanpool_rep, meanpool_rep.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[ 2.5316024 ,  0.5763005 , -0.4526847 , ..., -1.149106  ,\n",
              "                0.42457116,  0.7288165 ],\n",
              "              [ 3.0091994 ,  0.10912873, -2.029143  , ...,  0.4267935 ,\n",
              "               -0.6992538 , -1.1764154 ],\n",
              "              [ 3.9609914 , -0.72931445, -0.3609219 , ..., -0.32647783,\n",
              "                0.31873178, -1.5003738 ],\n",
              "              ...,\n",
              "              [ 3.1774433 ,  0.38270026,  0.0711159 , ...,  0.99890995,\n",
              "                0.65481365, -0.5698372 ],\n",
              "              [ 2.0822139 , -1.0173064 , -0.34569696, ..., -1.0563655 ,\n",
              "                0.271726  , -1.8396986 ],\n",
              "              [ 2.8549628 ,  0.4119694 , -1.1952059 , ..., -0.6041151 ,\n",
              "               -0.48218444, -0.39345092]], dtype=float32), (256, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auuj0BNZbYrX",
        "colab_type": "code",
        "outputId": "c16090a5-c8f2-4274-ad82-35a60a7b82a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "# max pool representation\n",
        "maxpool_model, cache_def = create_maxpool_model(init_rng, input_shape, transformer_kwargs)\n",
        "maxpool_rep = maxpool_model(random_seq)\n",
        "maxpool_rep, maxpool_rep.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[6.994354  , 3.6359208 , 3.1348662 , ..., 1.3081558 ,\n",
              "               2.4149635 , 4.2994876 ],\n",
              "              [5.9084325 , 2.918754  , 0.72082376, ..., 2.9975684 ,\n",
              "               2.2597349 , 1.2800888 ],\n",
              "              [6.1720195 , 2.3599405 , 1.8392863 , ..., 3.1160927 ,\n",
              "               2.2998276 , 1.2800888 ],\n",
              "              ...,\n",
              "              [6.399611  , 3.3775795 , 3.4750326 , ..., 4.10703   ,\n",
              "               3.3864174 , 2.1206608 ],\n",
              "              [5.517685  , 1.3853076 , 2.6230617 , ..., 2.4701836 ,\n",
              "               2.3067975 , 1.6047385 ],\n",
              "              [5.920331  , 2.8735316 , 2.2568762 , ..., 1.6873418 ,\n",
              "               1.8326222 , 1.7350802 ]], dtype=float32), (256, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDtHZM4Lc5Ey",
        "colab_type": "code",
        "outputId": "cf0be81f-9705-443c-c4b3-c2c6417c0e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "# linear + ReLU + max pool representation\n",
        "linearmaxpool_model, cache_def = create_linearmaxpool_model(init_rng, input_shape, transformer_kwargs)\n",
        "linearmaxpool_rep = linearmaxpool_model(random_seq)\n",
        "linearmaxpool_rep, linearmaxpool_rep.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[ 0.5269363 ,  0.572661  ,  5.065858  , ...,  0.04259292,\n",
              "                4.317007  , 10.2986355 ],\n",
              "              [ 2.19435   ,  0.52029675,  5.065858  , ...,  1.3588239 ,\n",
              "                5.387112  , 10.2986355 ],\n",
              "              [ 0.9113823 ,  2.4290025 ,  5.065858  , ...,  0.        ,\n",
              "                4.9920993 , 10.2986355 ],\n",
              "              ...,\n",
              "              [ 0.44557756,  1.8385924 ,  5.065858  , ...,  1.6889946 ,\n",
              "                4.2377467 , 10.2986355 ],\n",
              "              [ 1.0941364 ,  3.447383  ,  7.038131  , ...,  0.        ,\n",
              "                4.3995337 , 10.2986355 ],\n",
              "              [ 1.1540705 ,  2.104904  ,  6.480138  , ...,  0.4855646 ,\n",
              "                3.9888608 , 10.2986355 ]], dtype=float32), (256, 256))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WspJK-hc7ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}