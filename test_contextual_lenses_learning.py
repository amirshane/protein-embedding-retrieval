# -*- coding: utf-8 -*-
"""test_contextual_lenses_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PykmTCyuETlX4UFqoFCRpAJLIAxbAyg2

# Imports
"""

import functools
import itertools
import os
import time

import flax
from flax import jax_utils
from flax import nn
from flax import optim

import jax
from jax import random
from jax import lax
import jax.nn
import jax.numpy as jnp

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import scipy.stats

import sklearn.linear_model
import sklearn.metrics

import seaborn as sns

"""# Contextual lenses"""

def max_pool(x, rep_size=None):
  # Take maximum over sequence length

  rep = jnp.max(x, axis=-2)
   
  return rep


def mean_pool(x, rep_size=None):
  # Take mean over sequence length

  rep = jnp.mean(x, axis=-2)
  
  return rep


def linear_max_pool(x, rep_size=256):
  # Apply linear transformation + ReLU
  # Take maximum over sequence length
  
  x = nn.Dense(
        x,
        rep_size,
        kernel_init=nn.initializers.xavier_uniform(),
        bias_init=nn.initializers.normal(stddev=1e-6))
  
  x = nn.relu(x)
  
  rep = max_pool(x, rep_size)
  
  return rep


def linear_mean_pool(x, rep_size=256):
  # Apply linear transformation + ReLU
  # Take mean over sequence length
  
  x = nn.Dense(
        x,
        rep_size,
        kernel_init=nn.initializers.xavier_uniform(),
        bias_init=nn.initializers.normal(stddev=1e-6))
  
  x = nn.relu(x)
  
  rep = mean_pool(x, rep_size)
  
  return rep

"""# Models"""

def create_optimizer(model, learning_rate, weight_decay):
  optimizer_def = optim.Adam(learning_rate=learning_rate, weight_decay=weight_decay)
  optimizer = optimizer_def.create(model)
  return optimizer

@jax.jit
def train_step(optimizer, X, Y):
  def loss_fn(model):
    Y_hat = model(X)
    loss = jnp.mean((Y-Y_hat)**2)
    return loss
  grad_fn = jax.grad(loss_fn)
  grad = grad_fn(optimizer.target)
  optimizer = optimizer.apply_gradient(grad)
  return optimizer

"""## Encoder functions"""

class CNN(nn.Module):
  """A simple CNN model."""

  def apply(self, x, N_layers, N_features, kernel_sizes):
    
    x = jnp.expand_dims(x, axis=2)

    for i in range(N_layers):
      features = N_features[i]
      kernel_size = (kernel_sizes[i], 1)
      x = nn.Conv(x, features=features, kernel_size=kernel_size)
      x = nn.relu(x)
    
    x = x[:, :, 0, :]

    return x

def cnn_encoder(inputs, **encoder_fn_kwargs):
  N_layers, N_features, kernel_sizes = list(encoder_fn_kwargs.values())
  cnn_outputs = CNN(inputs, N_layers, N_features, kernel_sizes)
  return cnn_outputs

"""### Positional embeddings
Code source: https://github.com/google/flax/blob/aff10f032e892e28a1acf4dd4ee9dcc6cd39a606/examples/wmt/models.pyZ
"""

def sinusoidal_init(max_len=2048,
                    min_scale=1.0,
                    max_scale=10000.0):
  """1D Sinusoidal Position Embedding Initializer.
  Args:
      max_len: maximum possible length for the input.
      min_scale: float: minimum frequency-scale in sine grating.
      max_scale: float: maximum frequency-scale in sine grating.
  Returns:
      output: init function returning `(1, max_len, d_feature)`
  """

  def init(key, shape, dtype=np.float32):
    """Sinusoidal init."""
    del key, dtype
    d_feature = shape[-1]
    pe = np.zeros((max_len, d_feature), dtype=np.float32)
    position = np.arange(0, max_len)[:, np.newaxis]
    scale_factor = -np.log(max_scale / min_scale) / (d_feature // 2 - 1)
    div_term = min_scale * np.exp(np.arange(0, d_feature // 2) * scale_factor)
    pe[:, :d_feature // 2] = np.sin(position * div_term)
    pe[:, d_feature // 2: 2 * (d_feature // 2)] = np.cos(position * div_term)
    pe = pe[np.newaxis, :, :]  # [1, max_len, d_feature]
    return jnp.array(pe)

  return init

class AddPositionEmbs(nn.Module):
  """Adds (optionally learned) positional embeddings to the inputs."""

  def apply(self,
            inputs,
            inputs_positions=None,
            max_len=512,
            posemb_init=None,
            cache=None):
    """Applies AddPositionEmbs module.
    By default this layer uses a fixed sinusoidal embedding table. If a
    learned position embedding is desired, pass an initializer to
    posemb_init.
    Args:
      inputs: input data.
      inputs_positions: input position indices for packed sequences.
      max_len: maximum possible length for the input.
      posemb_init: positional embedding initializer, if None, then use a
        fixed (non-learned) sinusoidal embedding table.
      cache: flax attention cache for fast decoding.
    Returns:
      output: `(bs, timesteps, in_dim)`
    """
    # inputs.shape is (batch_size, seq_len, emb_dim)
    assert inputs.ndim == 3, ('Number of dimensions should be 3,'
                              ' but it is: %d' % inputs.ndim)
    length = inputs.shape[1]
    pos_emb_shape = (1, max_len, inputs.shape[-1])
    if posemb_init is None:
      # Use a fixed (non-learned) sinusoidal position embedding.
      pos_embedding = sinusoidal_init(
          max_len=max_len)(None, pos_emb_shape, None)
    else:
      pos_embedding = self.param('pos_embedding', pos_emb_shape, posemb_init)
    pe = pos_embedding[:, :length, :]
    # We abuse the same attention Cache mechanism to run positional embeddings
    # in fast predict mode. We could use state variables instead, but this
    # simplifies invocation with a single top-level cache context manager.
    # We only use the cache's position index for tracking decoding position.
    if cache:
      if self.is_initializing():
        cache.store(lambda: (4, (1, 1)))
      else:
        cache_entry = cache.retrieve(None)
        i = cache_entry.i
        cache.store(cache_entry.replace(i=cache_entry.i + 1))
        _, _, df = pos_embedding.shape
        pe = lax.dynamic_slice(pos_embedding,
                               jnp.array((0, i, 0)),
                               jnp.array((1, 1, df)))
    if inputs_positions is None:
      # normal unpacked case:
      return inputs + pe
    else:
      # for packed data we need to use known position indices:
      return inputs + jnp.take(pe[0], inputs_positions, axis=0)

def pos_emb_encoder(inputs, **encoder_fn_kwargs):
  max_len = list(encoder_fn_kwargs.values())[0]
  pos_emb = AddPositionEmbs(inputs, max_len=max_len)
  return pos_emb

def cnn_pos_emb_encoder(inputs, **encoder_fn_kwargs):
  N_layers, N_features, kernel_sizes, max_len = list(encoder_fn_kwargs.values())
  pos_emb = pos_emb_encoder(inputs, max_len=max_len)
  cnn_pos_emb = CNN(pos_emb, N_layers, N_features, kernel_sizes)
  return cnn_pos_emb

"""## Representation model"""

class TestRepresentationModel(nn.Module):

  def apply(self, x, encoder_fn, reduce_fn, seq_len, rep_size, linear_rep_size,
            **encoder_fn_kwargs):
    # Encode indices using embeddings
    # Apply lensing operation
    # Predict value

    if encoder_fn is not None:
        x = encoder_fn(x, **encoder_fn_kwargs)

    rep = reduce_fn(x, linear_rep_size)

    out = nn.Dense(rep,
                   1,
                   kernel_init=nn.initializers.xavier_uniform(),
                   bias_init=nn.initializers.normal(stddev=1e-6))
    
    return out

def create_test_representation_model(encoder_fn, reduce_fn, seq_len, rep_size,
                                     linear_rep_size=256,
                                     key=random.PRNGKey(0),
                                **encoder_fn_kwargs):
  module = TestRepresentationModel.partial(encoder_fn=encoder_fn, 
                                       reduce_fn=reduce_fn,
                                       rep_size=rep_size,
                                       seq_len=seq_len,
                                       linear_rep_size=linear_rep_size,
                                       **encoder_fn_kwargs)
  _, initial_params = TestRepresentationModel.init_by_shape(key,
                                                     input_specs=[((1, seq_len, rep_size), jnp.float32)],
                                                     encoder_fn=encoder_fn,
                                                     reduce_fn=reduce_fn,
                                                     rep_size=rep_size, 
                                                     seq_len=seq_len,
                                                     linear_rep_size=linear_rep_size,
                                                     **encoder_fn_kwargs)
  model = nn.Model(module, initial_params)
  return model

def create_optimizer(model, learning_rate, weight_decay):
  optimizer_def = optim.Adam(learning_rate=learning_rate, weight_decay=weight_decay)
  optimizer = optimizer_def.create(model)
  return optimizer

@jax.jit
def train_step(optimizer, X, Y):
  def loss_fn(model):
    Y_hat = model(X)
    Y_hat = jnp.squeeze(Y_hat, axis=1)
    loss = jnp.mean(jnp.square(Y-Y_hat))
    return loss
  grad_fn = jax.value_and_grad(loss_fn)
  _, grad = grad_fn(optimizer.target)
  optimizer = optimizer.apply_gradient(grad)
  return optimizer

"""# Tests

## Data
"""

bs = 5
seq_len = 4
rep_size = 10
input_data = jnp.array(np.random.normal(scale=5, size=(bs, seq_len, rep_size)))
output_data = jnp.array(np.random.normal(size=(bs)))

loss_threshold = 1e-4

def compute_train_loss(model, input_data, output_data, lr=1e-3):
  
  optimizer = create_optimizer(model, learning_rate=lr, weight_decay=0.)
  
  epochs = 150
  for epoch in range(epochs):
    optimizer = train_step(optimizer, input_data, output_data)

  preds = jnp.squeeze(optimizer.target(input_data), axis=1)

  train_loss = jnp.mean((preds-output_data)**2)

  return train_loss

"""## No encoding"""

def test_mean():
  
  encoder_fn = None
  reduce_fn = mean_pool
  
  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size)
  
  train_loss = compute_train_loss(model, input_data, output_data, lr=1e-1)

  assert train_loss < loss_threshold

def test_max():
  
  encoder_fn = None
  reduce_fn = max_pool
  
  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size)
  
  train_loss = compute_train_loss(model, input_data, output_data, lr=1e-1)

  assert train_loss < loss_threshold

def test_linear_max():
  
  encoder_fn = None
  reduce_fn = linear_max_pool
  linear_rep_size = 256
  
  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           linear_rep_size=linear_rep_size)
  
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

def test_linear_mean():
  
  encoder_fn = None
  reduce_fn = linear_mean_pool
  linear_rep_size = 256
  
  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           linear_rep_size=linear_rep_size)
    
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

"""## CNN"""

def test_cnn_mean():
  
  encoder_fn = cnn_encoder
  reduce_fn = mean_pool
  N_layers = 1
  N_features = [128]
  kernel_sizes = [2]

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           N_layers=N_layers,
                                           N_features=N_features,
                                           kernel_sizes=kernel_sizes)
  
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

def test_cnn_max():
  
  encoder_fn = cnn_encoder
  reduce_fn = max_pool
  N_layers = 1
  N_features = [128]
  kernel_sizes = [2]

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           N_layers=N_layers,
                                           N_features=N_features,
                                           kernel_sizes=kernel_sizes)
  
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

def test_cnn_linear_max():
  
  encoder_fn = cnn_encoder
  reduce_fn = linear_max_pool
  N_layers = 1
  N_features = [128]
  kernel_sizes = [2]
  linear_rep_size = 256

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           linear_rep_size=linear_rep_size,
                                           N_layers=N_layers,
                                           N_features=N_features,
                                           kernel_sizes=kernel_sizes)
  
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

def test_cnn_linear_mean():
  
  encoder_fn = cnn_encoder
  reduce_fn = linear_mean_pool
  N_layers = 1
  N_features = [128]
  kernel_sizes = [2]
  linear_rep_size = 256

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           linear_rep_size=linear_rep_size,
                                           N_layers=N_layers,
                                           N_features=N_features,
                                           kernel_sizes=kernel_sizes)
  
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

"""## Positional embedding"""

def test_pos_emb_mean():
  
  encoder_fn = pos_emb_encoder
  reduce_fn = mean_pool
  max_len = 512

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           max_len=max_len)
    
  train_loss = compute_train_loss(model, input_data, output_data, lr=1e-1)

  assert train_loss < loss_threshold

def test_pos_emb_max():
  
  encoder_fn = pos_emb_encoder
  reduce_fn = max_pool
  max_len = 512

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           max_len=max_len)
    
  train_loss = compute_train_loss(model, input_data, output_data, lr=1e-1)

  assert train_loss < loss_threshold

def test_pos_emb_linear_max():
  
  encoder_fn = pos_emb_encoder
  reduce_fn = linear_max_pool
  max_len = 512

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           max_len=max_len)
    
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

def test_pos_emb_linear_mean():
  
  encoder_fn = pos_emb_encoder
  reduce_fn = linear_mean_pool
  max_len = 512

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           max_len=max_len)
    
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

"""## Positional embedding + CNN"""

def test_cnn_pos_emb_mean():
  
  encoder_fn = cnn_pos_emb_encoder
  reduce_fn = mean_pool
  N_layers = 1
  N_features = [128]
  kernel_sizes = [2]
  max_len = 512

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           N_layers=N_layers,
                                           N_features=N_features,
                                           kernel_sizes=kernel_sizes,
                                           max_len=max_len)
  
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

def test_cnn_pos_emb_max():
  
  encoder_fn = cnn_pos_emb_encoder
  reduce_fn = max_pool
  N_layers = 1
  N_features = [128]
  kernel_sizes = [2]
  max_len = 512

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           N_layers=N_layers,
                                           N_features=N_features,
                                           kernel_sizes=kernel_sizes,
                                           max_len=max_len)
  
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

test_cnn_pos_emb_max()

def test_cnn_pos_emb_linear_max():
  
  encoder_fn = cnn_pos_emb_encoder
  reduce_fn = linear_max_pool
  N_layers = 1
  N_features = [128]
  kernel_sizes = [2]
  max_len = 512
  linear_rep_size = 256

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           linear_rep_size=linear_rep_size,
                                           N_layers=N_layers,
                                           N_features=N_features,
                                           kernel_sizes=kernel_sizes,
                                           max_len=max_len)
  
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

def test_cnn_pos_emb_linear_mean():
  
  encoder_fn = cnn_pos_emb_encoder
  reduce_fn = linear_mean_pool
  N_layers = 1
  N_features = [128]
  kernel_sizes = [2]
  max_len = 512
  linear_rep_size = 256

  model = create_test_representation_model(encoder_fn=encoder_fn,
                                           reduce_fn=reduce_fn,
                                           seq_len=seq_len,
                                           rep_size=rep_size,
                                           linear_rep_size=linear_rep_size,
                                           N_layers=N_layers,
                                           N_features=N_features,
                                           kernel_sizes=kernel_sizes,
                                           max_len=max_len)
  
  train_loss = compute_train_loss(model, input_data, output_data)

  assert train_loss < loss_threshold

