{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_contextual_lenses.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6JYlRZdKjtN",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jddAJf2QJqf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *** Comment out on cloud ***\n",
        "# Install the newest JAX and FLAX versions.\n",
        "!pip install --upgrade -q jax==0.1.61 jaxlib==0.1.42 flax==0.1.0rc2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEsdIj6XJ97M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "04528c27-1974-477a-a8c1-52297a324a97"
      },
      "source": [
        "# *** Comment out on cloud ***\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver_nightly'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grpc://10.98.69.2:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukKObz32J994",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "import itertools\n",
        "import os\n",
        "import time\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "from flax import nn\n",
        "from flax import optim\n",
        "from flax.metrics import tensorboard\n",
        "from flax.training import checkpoints\n",
        "from flax.training import common_utils\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "from jax import lax\n",
        "import jax.nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaurcJC1Pd1Y",
        "colab_type": "text"
      },
      "source": [
        "# Transformer model\n",
        "Code source: https://github.com/google/flax/blob/master/examples/lm1b/models.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FA6x8sNSvD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shift_right(x):\n",
        "  \"\"\"Shift the input to the right by padding on axis 1.\"\"\"\n",
        "  pad_widths = [(0, 0)] * len(x.shape)\n",
        "  pad_widths[1] = (1, 0)  # Padding on axis=1\n",
        "  padded = jnp.pad(\n",
        "      x, pad_widths, mode='constant', constant_values=x.dtype.type(0))\n",
        "  return padded[:, :-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhlpIkvGTCN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embed(nn.Module):\n",
        "  \"\"\"Embedding Module.\n",
        "  A parameterized function from integers [0, n) to d-dimensional vectors.\n",
        "  \"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            num_embeddings,\n",
        "            features,\n",
        "            mode='input',\n",
        "            emb_init=nn.initializers.normal(stddev=1.0)):\n",
        "    \"\"\"Applies Embed module.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      num_embeddings: number of embedding\n",
        "      features: size of the embedding dimension\n",
        "      mode: either 'input' or 'output' -> to share input/output embedding\n",
        "      emb_init: embedding initializer\n",
        "    Returns:\n",
        "      output which is embedded input data\n",
        "    \"\"\"\n",
        "    embedding = self.param('embedding', (num_embeddings, features), emb_init)\n",
        "    if mode == 'input':\n",
        "      if inputs.dtype not in [jnp.int32, jnp.int64, jnp.uint32, jnp.uint64]:\n",
        "        raise ValueError('Input type must be an integer or unsigned integer.')\n",
        "      return jnp.take(embedding, inputs, axis=0)\n",
        "    if mode == 'output':\n",
        "      return jnp.einsum('bld,vd->blv', inputs, embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeZlvb5IT0Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sinusoidal_init(max_len=2048):\n",
        "  \"\"\"1D Sinusoidal Position Embedding Initializer.\n",
        "  Args:\n",
        "      max_len: maximum possible length for the input\n",
        "  Returns:\n",
        "      output: init function returning `(1, max_len, d_feature)`\n",
        "  \"\"\"\n",
        "\n",
        "  def init(key, shape, dtype=np.float32):\n",
        "    \"\"\"Sinusoidal init.\"\"\"\n",
        "    del key, dtype\n",
        "    d_feature = shape[-1]\n",
        "    pe = np.zeros((max_len, d_feature), dtype=np.float32)\n",
        "    position = np.arange(0, max_len)[:, np.newaxis]\n",
        "    div_term = np.exp(\n",
        "        np.arange(0, d_feature, 2) * -(np.log(10000.0) / d_feature))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    pe = pe[np.newaxis, :, :]  # [1, max_len, d_feature]\n",
        "    return jnp.array(pe)\n",
        "\n",
        "  return init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL7EOFcLT0W5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AddPositionEmbs(nn.Module):\n",
        "  \"\"\"Adds learned positional embeddings to the inputs.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            max_len=2048,\n",
        "            posemb_init=nn.initializers.normal(stddev=1.0),\n",
        "            cache=None):\n",
        "    \"\"\"Applies AddPositionEmbs module.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      max_len: maximum possible length for the input\n",
        "      posemb_init: positional embedding initializer\n",
        "      cache: flax attention cache for fast decoding.\n",
        "    Returns:\n",
        "      output: `(bs, timesteps, in_dim)`\n",
        "    \"\"\"\n",
        "    assert inputs.ndim == 3, ('Number of dimensions should be 3,'\n",
        "                              ' but it is: %d' % inputs.ndim)\n",
        "    length = inputs.shape[1]\n",
        "    pos_emb_shape = (1, max_len, inputs.shape[-1])\n",
        "    pos_embedding = self.param('pos_embedding', pos_emb_shape, posemb_init)\n",
        "    pe = pos_embedding[:, :length, :]\n",
        "    # We abuse the same attention Cache mechanism to run positional embeddings\n",
        "    # in fast predict mode. We could use state variables instead, but this\n",
        "    # simplifies invocation with a single top-level cache context manager.\n",
        "    # We only use the cache's position index for tracking decoding position.\n",
        "    if cache:\n",
        "      if self.is_initializing():\n",
        "        cache.store(lambda: (4, (1, 1)))\n",
        "      else:\n",
        "        cache_entry = cache.retrieve(None)\n",
        "        i = cache_entry.i\n",
        "        one = jnp.array(1, jnp.uint32)\n",
        "        cache_entry = cache_entry.replace(i=cache_entry.i + one)\n",
        "        cache.store(cache_entry)\n",
        "        _, _, df = pos_embedding.shape\n",
        "        pe = lax.dynamic_slice(pos_embedding, jnp.array((0, i, 0)),\n",
        "                               jnp.array((1, 1, df)))\n",
        "    return inputs + pe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nmqhnGGVIba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MlpBlock(nn.Module):\n",
        "  \"\"\"Transformer MLP block.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            mlp_dim,\n",
        "            out_dim=None,\n",
        "            dropout_rate=0.1,\n",
        "            deterministic=False,\n",
        "            kernel_init=nn.initializers.xavier_uniform(),\n",
        "            bias_init=nn.initializers.normal(stddev=1e-6)):\n",
        "    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n",
        "    actual_out_dim = inputs.shape[-1] if out_dim is None else out_dim\n",
        "    x = nn.Dense(inputs, mlp_dim, kernel_init=kernel_init, bias_init=bias_init)\n",
        "    x = nn.gelu(x)\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)\n",
        "    output = nn.Dense(\n",
        "        x, actual_out_dim, kernel_init=kernel_init, bias_init=bias_init)\n",
        "    output = nn.dropout(output, rate=dropout_rate, deterministic=deterministic)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHmusZUHVQAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer1DBlock(nn.Module):\n",
        "  \"\"\"Transformer layer (https://openreview.net/forum?id=H1e5GJBtDr).\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            qkv_dim,\n",
        "            mlp_dim,\n",
        "            num_heads,\n",
        "            causal_mask=False,\n",
        "            padding_mask=None,\n",
        "            dropout_rate=0.1,\n",
        "            attention_dropout_rate=0.1,\n",
        "            deterministic=False,\n",
        "            cache=None):\n",
        "    \"\"\"Applies Transformer1DBlock module.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      qkv_dim: dimension of the query/key/value\n",
        "      mlp_dim: dimension of the mlp on top of attention block\n",
        "      num_heads: number of heads\n",
        "      causal_mask: bool, mask future or not\n",
        "      padding_mask: bool, mask padding tokens\n",
        "      dropout_rate: dropout rate\n",
        "      attention_dropout_rate: dropout rate for attention weights\n",
        "      deterministic: bool, deterministic or not (to apply dropout)\n",
        "      cache: flax autoregressive cache for fast decoding.\n",
        "    Returns:\n",
        "      output after transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    # Attention block.\n",
        "    assert inputs.ndim == 3\n",
        "    x = nn.LayerNorm(inputs)\n",
        "    x = nn.SelfAttention(\n",
        "        x,\n",
        "        num_heads=num_heads,\n",
        "        qkv_features=qkv_dim,\n",
        "        attention_axis=(1,),\n",
        "        causal_mask=causal_mask,\n",
        "        padding_mask=padding_mask,\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.normal(stddev=1e-6),\n",
        "        bias=False,\n",
        "        broadcast_dropout=False,\n",
        "        dropout_rate=attention_dropout_rate,\n",
        "        deterministic=deterministic,\n",
        "        cache=cache)\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)\n",
        "    x = x + inputs\n",
        "\n",
        "    # MLP block.\n",
        "    y = nn.LayerNorm(x)\n",
        "    y = MlpBlock(\n",
        "        y,\n",
        "        mlp_dim=mlp_dim,\n",
        "        dropout_rate=dropout_rate,\n",
        "        deterministic=deterministic)\n",
        "\n",
        "    return x + y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVbX9CupVIhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yJSyWhhE2U7",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hatWMbzmErS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train_steps = 500000      # Max number of training steps.\n",
        "eval_frequency = 1000         # How often to run model evaluation.\n",
        "num_eval_steps = 20           # Number of steps to take during evaluation.\n",
        "random_seed = 0               # JAX PRNG random seed.\n",
        "learning_rate = 0.05          # Base learning rate.\n",
        "weight_decay = 1e-1           # AdamW-style relative weight decay factor.\n",
        "batch_size = 256              # \"Target\" Batch size.\n",
        "max_target_length = 256       # Maximum input length.\n",
        "max_eval_target_length = 256  # Maximum eval-set input length.\n",
        "\n",
        "lm_emb_dim = 512              # LM initial token embedding dimension.\n",
        "lm_num_heads = 8              # Number of heads in decoder layers.\n",
        "lm_num_layers = 6             # Number of decoder layers.\n",
        "lm_qkv_dim = 512              # Decoder query/key/value depth.\n",
        "lm_mlp_dim = 2048             # Feedforward (MLP) layer depth.\n",
        "\n",
        "rep_size = 256                 # Size of learned linear representation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mePIEKElffK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Init PRNG Stream.\n",
        "rng = random.PRNGKey(random_seed)\n",
        "rng, init_rng = random.split(rng)\n",
        "# We init the first set of dropout PRNG keys, but update it afterwards inside\n",
        "# the main pmap'd training update for performance.\n",
        "dropout_rngs = random.split(rng, jax.local_device_count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj0vI3LmVnvj",
        "colab_type": "text"
      },
      "source": [
        "# Transformer language model\n",
        "Code source: https://github.com/google/flax/blob/master/examples/lm1b/models.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsGiLbDSKwDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerLM(nn.Module):\n",
        "  \"\"\"Transformer Model for language modeling.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            vocab_size,\n",
        "            emb_dim=512,\n",
        "            num_heads=8,\n",
        "            num_layers=6,\n",
        "            qkv_dim=512,\n",
        "            mlp_dim=2048,\n",
        "            max_len=2048,\n",
        "            train=False,\n",
        "            shift=True,\n",
        "            dropout_rate=0.1,\n",
        "            attention_dropout_rate=0.1,\n",
        "            cache=None):\n",
        "    \"\"\"Applies Transformer model on the inputs.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      vocab_size: size of the vocabulary\n",
        "      emb_dim: dimension of embedding\n",
        "      num_heads: number of heads\n",
        "      num_layers: number of layers\n",
        "      qkv_dim: dimension of the query/key/value\n",
        "      mlp_dim: dimension of the mlp on top of attention block\n",
        "      max_len: maximum length.\n",
        "      train: bool: if model is training.\n",
        "      shift: bool: if we right-shift input - this is only disabled for\n",
        "        fast, looped single-token autoregressive decoding.\n",
        "      dropout_rate: dropout rate\n",
        "      attention_dropout_rate: dropout rate for attention weights\n",
        "      cache: flax autoregressive cache for fast decoding.\n",
        "    Returns:\n",
        "      output of a transformer decoder.\n",
        "    \"\"\"\n",
        "    padding_mask = jnp.where(inputs > 0, 1, 0).astype(jnp.float32)[..., None]\n",
        "    assert inputs.ndim == 2  # (batch, len)\n",
        "    x = inputs\n",
        "    if shift:\n",
        "      x = shift_right(x)\n",
        "    x = x.astype('int32')\n",
        "\n",
        "    x = Embed(x, num_embeddings=vocab_size, features=emb_dim, name='embed')\n",
        "\n",
        "    x = AddPositionEmbs(\n",
        "        x, max_len=max_len, posemb_init=sinusoidal_init(max_len=max_len),\n",
        "        cache=cache)\n",
        "\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=not train)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      x = Transformer1DBlock(\n",
        "          x,\n",
        "          qkv_dim=qkv_dim,\n",
        "          mlp_dim=mlp_dim,\n",
        "          num_heads=num_heads,\n",
        "          causal_mask=True,\n",
        "          padding_mask=padding_mask,\n",
        "          dropout_rate=dropout_rate,\n",
        "          attention_dropout_rate=attention_dropout_rate,\n",
        "          deterministic=not train,\n",
        "          cache=cache,\n",
        "      )\n",
        "\n",
        "    x = nn.LayerNorm(x)\n",
        "\n",
        "    logits = nn.Dense(\n",
        "        x,\n",
        "        vocab_size,\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.normal(stddev=1e-6))\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqAXwLopL5Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdyyxXnjnXh5",
        "colab_type": "text"
      },
      "source": [
        "# Transformer lenses\n",
        "Based on: https://arxiv.org/pdf/2002.08866.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucSiuMvhniqX",
        "colab_type": "text"
      },
      "source": [
        "### Lens 1: Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRSnZknTFDcv",
        "colab_type": "text"
      },
      "source": [
        "Mean pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EhGsQM2s8l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerMeanPool(nn.Module):\n",
        "  \"\"\"Transformer Model + mean pooling for representations.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            vocab_size,\n",
        "            emb_dim=512,\n",
        "            num_heads=8,\n",
        "            num_layers=6,\n",
        "            qkv_dim=512,\n",
        "            mlp_dim=2048,\n",
        "            max_len=2048,\n",
        "            train=False,\n",
        "            shift=True,\n",
        "            dropout_rate=0.1,\n",
        "            attention_dropout_rate=0.1,\n",
        "            cache=None):\n",
        "    \"\"\"Applies Transformer model on the inputs.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      vocab_size: size of the vocabulary\n",
        "      emb_dim: dimension of embedding\n",
        "      num_heads: number of heads\n",
        "      num_layers: number of layers\n",
        "      qkv_dim: dimension of the query/key/value\n",
        "      mlp_dim: dimension of the mlp on top of attention block\n",
        "      max_len: maximum length.\n",
        "      train: bool: if model is training.\n",
        "      shift: bool: if we right-shift input - this is only disabled for\n",
        "        fast, looped single-token autoregressive decoding.\n",
        "      dropout_rate: dropout rate\n",
        "      attention_dropout_rate: dropout rate for attention weights\n",
        "      cache: flax autoregressive cache for fast decoding.\n",
        "    Returns:\n",
        "      output of a transformer decoder.\n",
        "    \"\"\"\n",
        "    padding_mask = jnp.where(inputs > 0, 1, 0).astype(jnp.float32)[..., None]\n",
        "    assert inputs.ndim == 2  # (batch, len)\n",
        "    x = inputs\n",
        "    if shift:\n",
        "      x = shift_right(x)\n",
        "    x = x.astype('int32')\n",
        "\n",
        "    x = Embed(x, num_embeddings=vocab_size, features=emb_dim, name='embed')\n",
        "\n",
        "    x = AddPositionEmbs(\n",
        "        x, max_len=max_len, posemb_init=sinusoidal_init(max_len=max_len),\n",
        "        cache=cache)\n",
        "\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=not train)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      x = Transformer1DBlock(\n",
        "          x,\n",
        "          qkv_dim=qkv_dim,\n",
        "          mlp_dim=mlp_dim,\n",
        "          num_heads=num_heads,\n",
        "          causal_mask=True,\n",
        "          padding_mask=padding_mask,\n",
        "          dropout_rate=dropout_rate,\n",
        "          attention_dropout_rate=attention_dropout_rate,\n",
        "          deterministic=not train,\n",
        "          cache=cache,\n",
        "      )\n",
        "\n",
        "    x = nn.LayerNorm(x)\n",
        "\n",
        "    rep = jnp.mean(x, axis=1)\n",
        "\n",
        "    return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6EN-0vJMEqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko3LAJLUFK7E",
        "colab_type": "text"
      },
      "source": [
        "Max pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS-hJUBCny8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerMaxPool(nn.Module):\n",
        "  \"\"\"Transformer Model + max pooling for representations.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            vocab_size,\n",
        "            emb_dim=512,\n",
        "            num_heads=8,\n",
        "            num_layers=6,\n",
        "            qkv_dim=512,\n",
        "            mlp_dim=2048,\n",
        "            max_len=2048,\n",
        "            train=False,\n",
        "            shift=True,\n",
        "            dropout_rate=0.1,\n",
        "            attention_dropout_rate=0.1,\n",
        "            cache=None):\n",
        "    \"\"\"Applies Transformer model on the inputs.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      vocab_size: size of the vocabulary\n",
        "      emb_dim: dimension of embedding\n",
        "      num_heads: number of heads\n",
        "      num_layers: number of layers\n",
        "      qkv_dim: dimension of the query/key/value\n",
        "      mlp_dim: dimension of the mlp on top of attention block\n",
        "      max_len: maximum length.\n",
        "      train: bool: if model is training.\n",
        "      shift: bool: if we right-shift input - this is only disabled for\n",
        "        fast, looped single-token autoregressive decoding.\n",
        "      dropout_rate: dropout rate\n",
        "      attention_dropout_rate: dropout rate for attention weights\n",
        "      cache: flax autoregressive cache for fast decoding.\n",
        "    Returns:\n",
        "      output of a transformer decoder.\n",
        "    \"\"\"\n",
        "    padding_mask = jnp.where(inputs > 0, 1, 0).astype(jnp.float32)[..., None]\n",
        "    assert inputs.ndim == 2  # (batch, len)\n",
        "    x = inputs\n",
        "    if shift:\n",
        "      x = shift_right(x)\n",
        "    x = x.astype('int32')\n",
        "\n",
        "    x = Embed(x, num_embeddings=vocab_size, features=emb_dim, name='embed')\n",
        "\n",
        "    x = AddPositionEmbs(\n",
        "        x, max_len=max_len, posemb_init=sinusoidal_init(max_len=max_len),\n",
        "        cache=cache)\n",
        "\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=not train)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      x = Transformer1DBlock(\n",
        "          x,\n",
        "          qkv_dim=qkv_dim,\n",
        "          mlp_dim=mlp_dim,\n",
        "          num_heads=num_heads,\n",
        "          causal_mask=True,\n",
        "          padding_mask=padding_mask,\n",
        "          dropout_rate=dropout_rate,\n",
        "          attention_dropout_rate=attention_dropout_rate,\n",
        "          deterministic=not train,\n",
        "          cache=cache,\n",
        "      )\n",
        "\n",
        "    x = nn.LayerNorm(x)\n",
        "\n",
        "    rep = jnp.max(x, axis=1)\n",
        "\n",
        "    return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7qRlwgVMLFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwduXidjni9l",
        "colab_type": "text"
      },
      "source": [
        "### Lens 2: Linear + ReLU + Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAF_K_vimV_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerLinearMaxPool(nn.Module):\n",
        "  \"\"\"Transformer Model + linear layer + max pooling for representations.\"\"\"\n",
        "\n",
        "  def apply(self,\n",
        "            inputs,\n",
        "            vocab_size,\n",
        "            rep_size=256,\n",
        "            emb_dim=512,\n",
        "            num_heads=8,\n",
        "            num_layers=6,\n",
        "            qkv_dim=512,\n",
        "            mlp_dim=2048,\n",
        "            max_len=2048,\n",
        "            train=False,\n",
        "            shift=True,\n",
        "            dropout_rate=0.1,\n",
        "            attention_dropout_rate=0.1,\n",
        "            cache=None):\n",
        "    \"\"\"Applies Transformer model on the inputs.\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      vocab_size: size of the vocabulary\n",
        "      emb_dim: dimension of embedding\n",
        "      num_heads: number of heads\n",
        "      num_layers: number of layers\n",
        "      qkv_dim: dimension of the query/key/value\n",
        "      mlp_dim: dimension of the mlp on top of attention block\n",
        "      max_len: maximum length.\n",
        "      train: bool: if model is training.\n",
        "      shift: bool: if we right-shift input - this is only disabled for\n",
        "        fast, looped single-token autoregressive decoding.\n",
        "      dropout_rate: dropout rate\n",
        "      attention_dropout_rate: dropout rate for attention weights\n",
        "      cache: flax autoregressive cache for fast decoding.\n",
        "    Returns:\n",
        "      output of a transformer decoder.\n",
        "    \"\"\"\n",
        "    padding_mask = jnp.where(inputs > 0, 1, 0).astype(jnp.float32)[..., None]\n",
        "    assert inputs.ndim == 2  # (batch, len)\n",
        "    x = inputs\n",
        "    if shift:\n",
        "      x = shift_right(x)\n",
        "    x = x.astype('int32')\n",
        "\n",
        "    x = Embed(x, num_embeddings=vocab_size, features=emb_dim, name='embed')\n",
        "\n",
        "    x = AddPositionEmbs(\n",
        "        x, max_len=max_len, posemb_init=sinusoidal_init(max_len=max_len),\n",
        "        cache=cache)\n",
        "\n",
        "    x = nn.dropout(x, rate=dropout_rate, deterministic=not train)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      x = Transformer1DBlock(\n",
        "          x,\n",
        "          qkv_dim=qkv_dim,\n",
        "          mlp_dim=mlp_dim,\n",
        "          num_heads=num_heads,\n",
        "          causal_mask=True,\n",
        "          padding_mask=padding_mask,\n",
        "          dropout_rate=dropout_rate,\n",
        "          attention_dropout_rate=attention_dropout_rate,\n",
        "          deterministic=not train,\n",
        "          cache=cache,\n",
        "      )\n",
        "\n",
        "    x = nn.LayerNorm(x)\n",
        "\n",
        "    x = nn.Dense(\n",
        "        x,\n",
        "        rep_size,\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.normal(stddev=1e-6))\n",
        "    \n",
        "    x = nn.relu(x)\n",
        "    \n",
        "    rep = jnp.max(x, axis=1)\n",
        "\n",
        "    return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpC4ivhLMQge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTZwqhbabKMt",
        "colab_type": "text"
      },
      "source": [
        "## Test models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr4vSpB1bTZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_language_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Language Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerLM.partial(**model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_meanpool_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Representation Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerMeanPool.partial(**model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_maxpool_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Representation Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerMaxPool.partial(**model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def create_linearmaxpool_model(key, input_shape, model_kwargs):\n",
        "  \"\"\"\n",
        "  We create a model definition from the top-level Representation Model and \n",
        "  passed in hyperparameters.\n",
        "  \"\"\"\n",
        "  module = TransformerLinearMaxPool.partial(**model_kwargs)\n",
        "  # We initialize an autoregressive Cache collection for fast, autoregressive\n",
        "  # decoding through the language model's decoder layers.\n",
        "  with nn.attention.Cache().mutate() as cache_def:\n",
        "    # create_by_shape initializes the model parameters.\n",
        "    _, model = module.create_by_shape(key,\n",
        "                                         [(input_shape, jnp.float32)],\n",
        "                                         cache=cache_def)\n",
        "  return model, cache_def"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJSeW-RlbYm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 20\n",
        "\n",
        "input_shape = (batch_size, max_target_length)\n",
        "\n",
        "transformer_kwargs = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'emb_dim': lm_emb_dim,\n",
        "    'num_heads': lm_num_heads,\n",
        "    'num_layers': lm_num_layers,\n",
        "    'qkv_dim': lm_qkv_dim,\n",
        "    'mlp_dim': lm_mlp_dim,\n",
        "    'max_len': max(max_target_length, max_eval_target_length)\n",
        "}\n",
        "\n",
        "transformer_linear_kwargs = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'rep_size' : rep_size,\n",
        "    'emb_dim': lm_emb_dim,\n",
        "    'num_heads': lm_num_heads,\n",
        "    'num_layers': lm_num_layers,\n",
        "    'qkv_dim': lm_qkv_dim,\n",
        "    'mlp_dim': lm_mlp_dim,\n",
        "    'max_len': max(max_target_length, max_eval_target_length)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M30XKTOcJuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a random sequence\n",
        "random_seq_len = 32\n",
        "random_seq = jnp.array([[np.random.randint(vocab_size) for _ in range(random_seq_len)] for __ in range(batch_size)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLDJCHrKcJw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f6085f0-7cdb-4c17-c1a3-c334e51abda5"
      },
      "source": [
        "# language model\n",
        "language_model, cache_def = create_language_model(init_rng, input_shape, transformer_kwargs)\n",
        "logits = language_model(random_seq)\n",
        "logits, logits.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-3.0559456e-01, -2.4639273e+00,  2.3458025e+00, ...,\n",
              "                -6.5893048e-01, -1.9393097e+00, -8.4083211e-01],\n",
              "               [-4.4434652e-01, -1.5671092e+00,  3.6528590e+00, ...,\n",
              "                 9.1124940e-01, -3.7526426e+00, -3.5367021e-01],\n",
              "               ...,\n",
              "               [-1.3982043e+00, -1.5023800e+00,  8.0621487e-01, ...,\n",
              "                 8.2945955e-01, -9.7772032e-01, -1.6740191e+00],\n",
              "               [-1.8099425e+00, -1.3558569e+00,  1.3833988e+00, ...,\n",
              "                 1.2333223e+00, -1.7917389e-01, -5.7866496e-01],\n",
              "               [-5.6542236e-01, -2.0965300e+00,  1.4960983e+00, ...,\n",
              "                 9.0686068e-02, -9.1157734e-01,  5.9599954e-01]],\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-1.9407952e+00, -2.0996714e+00,  2.9088345e+00, ...,\n",
              "                 8.1890476e-01, -3.0424330e+00, -1.4295510e+00],\n",
              "               [-1.0459772e+00, -2.6079979e+00,  2.3576455e+00, ...,\n",
              "                 1.0978714e+00, -1.3766145e+00,  2.4475046e-03],\n",
              "               ...,\n",
              "               [-1.9375813e-01, -2.8480783e+00,  2.7969842e+00, ...,\n",
              "                 1.3755881e+00, -1.1696399e+00,  2.9927707e-01],\n",
              "               [-5.7494849e-01, -2.1998062e+00,  1.0092756e+00, ...,\n",
              "                 1.2185100e+00, -7.0885128e-01, -1.8974204e+00],\n",
              "               [ 1.2928433e+00, -3.3496022e+00,  2.3233151e+00, ...,\n",
              "                 1.2834148e+00,  1.6358510e+00,  7.9134572e-01]],\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-1.7104239e+00, -4.4559455e-01,  1.8239067e+00, ...,\n",
              "                 2.5488155e-02, -2.3136425e+00,  2.9076567e-01],\n",
              "               [-1.6086344e+00, -1.3957337e+00,  1.4463333e+00, ...,\n",
              "                -6.1698747e-01, -1.7829669e+00, -2.1296325e+00],\n",
              "               ...,\n",
              "               [-1.2091147e-01, -1.3447119e+00,  1.3235365e+00, ...,\n",
              "                 1.4447297e+00,  1.5007149e+00, -1.0251652e-02],\n",
              "               [-5.3801501e-01, -7.6847416e-01,  3.6907158e+00, ...,\n",
              "                 1.7261246e+00,  9.0565491e-01,  8.6109453e-01],\n",
              "               [-6.5607452e-01, -1.5967251e+00,  2.2293057e+00, ...,\n",
              "                 7.7540767e-01,  1.6388820e+00,  2.8291073e-01]],\n",
              " \n",
              "              ...,\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-6.1661464e-01, -1.2949111e+00,  2.4757829e+00, ...,\n",
              "                -1.3056217e+00, -9.0616316e-01,  1.3522960e-01],\n",
              "               [-1.0554404e+00, -1.9982935e+00,  2.9908025e+00, ...,\n",
              "                -1.0589850e+00, -2.2285171e+00, -1.2640046e+00],\n",
              "               ...,\n",
              "               [ 7.1905082e-01, -8.2870489e-01,  1.3375955e+00, ...,\n",
              "                 8.8064879e-01,  1.8400693e+00, -8.4643829e-01],\n",
              "               [-4.0928936e-01, -1.6371397e+00,  1.0155693e+00, ...,\n",
              "                -5.0485218e-01,  7.7137476e-01, -1.8226374e+00],\n",
              "               [ 1.9978473e-01, -2.0507867e+00,  2.5864151e+00, ...,\n",
              "                 8.2591736e-01,  1.0038927e+00,  3.1839570e-01]],\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-8.9981824e-01, -1.3519605e+00,  1.3797101e+00, ...,\n",
              "                 1.3774323e-02, -1.7791220e+00, -1.5944890e+00],\n",
              "               [-5.3526318e-01, -2.2448466e+00,  2.1595693e+00, ...,\n",
              "                 8.1504810e-01,  1.0195589e+00,  1.0371603e+00],\n",
              "               ...,\n",
              "               [ 1.0972555e+00, -1.9638227e+00,  9.4791561e-01, ...,\n",
              "                 4.5682919e-01,  2.0622120e+00, -8.0797029e-01],\n",
              "               [-1.0525956e+00, -1.9059380e+00,  2.0842092e+00, ...,\n",
              "                 9.3067282e-01,  1.3458556e+00,  8.2173717e-01],\n",
              "               [-1.0617537e+00, -1.4798335e+00,  1.5475278e+00, ...,\n",
              "                 1.0209422e+00,  1.7903031e+00,  1.0640635e-01]],\n",
              " \n",
              "              [[-4.5319790e-01, -1.3108684e+00,  2.1032448e+00, ...,\n",
              "                -1.0460795e+00, -1.6000499e+00, -2.6734672e+00],\n",
              "               [-2.2059345e+00, -1.3470999e+00,  2.1907320e+00, ...,\n",
              "                -1.5825154e+00, -1.9402893e+00,  5.2538699e-01],\n",
              "               [-1.6394243e+00, -6.6472697e-01,  1.2425531e+00, ...,\n",
              "                 3.8874173e-01, -5.5794418e-01,  1.1306419e-01],\n",
              "               ...,\n",
              "               [-1.2896483e+00, -7.6710528e-01,  7.0791864e-01, ...,\n",
              "                -3.2243311e-01,  3.9783084e-01, -4.4075096e-01],\n",
              "               [-1.1604815e+00, -6.1645681e-01,  6.7868644e-01, ...,\n",
              "                 1.5844114e+00, -7.6878883e-02,  2.3107250e+00],\n",
              "               [ 7.1070321e-02, -2.1488683e+00,  2.7295127e+00, ...,\n",
              "                 3.3105005e-02, -1.1693588e+00, -8.0168980e-01]]],            dtype=float32),\n",
              " (256, 32, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXcJhL6lcKY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "aead5cd6-eb79-4243-c461-69ed88130502"
      },
      "source": [
        "# mean pool representation\n",
        "meanpool_model, cache_def = create_meanpool_model(init_rng, input_shape, transformer_kwargs)\n",
        "meanpool_rep = meanpool_model(random_seq)\n",
        "meanpool_rep, meanpool_rep.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[ 1.349282  ,  0.16946445, -0.09409462, ..., -0.39269203,\n",
              "               -0.03846568, -0.96833426],\n",
              "              [ 1.42307   , -0.3462407 , -0.17818117, ...,  0.39621493,\n",
              "               -0.36760882, -0.42438346],\n",
              "              [ 0.9597972 , -0.34990683, -0.7753747 , ..., -0.04606063,\n",
              "               -0.79815584, -0.7785816 ],\n",
              "              ...,\n",
              "              [ 1.516348  , -0.7187285 , -0.1575165 , ..., -0.19013198,\n",
              "               -0.34239438, -0.8993825 ],\n",
              "              [ 0.872891  , -0.6433126 , -0.87928617, ..., -0.26089457,\n",
              "                0.07963648, -0.7745665 ],\n",
              "              [ 1.5188763 , -0.1361511 , -0.00225492, ...,  0.19809611,\n",
              "               -0.32082427, -1.0763489 ]], dtype=float32), (256, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auuj0BNZbYrX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "2d31600b-8239-4b8a-b8b6-5ef6dbe43ff1"
      },
      "source": [
        "# max pool representation\n",
        "maxpool_model, cache_def = create_maxpool_model(init_rng, input_shape, transformer_kwargs)\n",
        "maxpool_rep = maxpool_model(random_seq)\n",
        "maxpool_rep, maxpool_rep.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[ 2.5953405 ,  1.194404  ,  0.8714977 , ...,  1.4238741 ,\n",
              "                0.75553006,  0.47919446],\n",
              "              [ 2.6580777 ,  0.63564575,  1.0401169 , ...,  2.014348  ,\n",
              "                0.5694527 ,  0.92812794],\n",
              "              [ 2.1212747 ,  0.58689904,  0.36625844, ...,  0.9152341 ,\n",
              "               -0.04691934,  0.37015328],\n",
              "              ...,\n",
              "              [ 3.0935042 ,  0.37259296,  1.121785  , ...,  1.2526655 ,\n",
              "                0.831916  ,  0.27077362],\n",
              "              [ 2.3825467 ,  0.5376274 ,  0.20739071, ...,  1.0502782 ,\n",
              "                0.8913085 ,  0.6887138 ],\n",
              "              [ 2.5058422 ,  1.1807996 ,  1.4759212 , ...,  1.0194496 ,\n",
              "                0.54213744,  0.19198617]], dtype=float32), (256, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDtHZM4Lc5Ey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "2d0a0d93-df09-40e2-d658-d5c999306110"
      },
      "source": [
        "# linear + ReLU + max pool representation\n",
        "linearmaxpool_model, cache_def = create_linearmaxpool_model(init_rng, input_shape, transformer_linear_kwargs)\n",
        "linearmaxpool_rep = linearmaxpool_model(random_seq)\n",
        "linearmaxpool_rep, linearmaxpool_rep.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[0.844927  , 2.8515341 , 1.1309768 , ..., 0.92408   ,\n",
              "               1.3553803 , 2.94846   ],\n",
              "              [1.2444927 , 2.42132   , 0.48922083, ..., 1.1925056 ,\n",
              "               1.6753874 , 2.1524806 ],\n",
              "              [0.8767196 , 2.9770274 , 1.1056776 , ..., 0.5257038 ,\n",
              "               1.0089328 , 2.5957775 ],\n",
              "              ...,\n",
              "              [0.85885113, 2.5804234 , 0.8209575 , ..., 1.3480121 ,\n",
              "               1.244442  , 2.6098092 ],\n",
              "              [1.1995897 , 2.4142926 , 0.88203317, ..., 0.45323965,\n",
              "               0.56194437, 2.6673195 ],\n",
              "              [1.1544257 , 2.6171696 , 0.49701124, ..., 1.037597  ,\n",
              "               1.9722457 , 2.0675519 ]], dtype=float32), (256, 256))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WspJK-hc7ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}